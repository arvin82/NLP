{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9aa6f4",
   "metadata": {},
   "source": [
    "# Learning word embeddings\n",
    "\n",
    "There are several techniques or algorithms to learn or compute word vectors.\n",
    "The most famous word2vec techniques are:\n",
    "- Continous Bag Of Words (CBOW)\n",
    "- Skip-gram\n",
    "- Glove\n",
    "\n",
    "There are other techniques also based on attention models.\n",
    "\n",
    "- Continous Bag Of Words (CBOW):\n",
    "\n",
    "In this technique, n words before and after a word are considered. Their one-hot vectors are averaged and input to a NN with input size of V, where V is the size of the vocabulary. There is only one hidden layer in the NN and the size of that, N, is assigned to the desired size of learnt word vectors. The output of the NN is also of size V. The output is softmaxed and is compared with one-hot vector under training. The cost function is created based on cross-entropy. The weight matrices will be of size N x V and can be used to extract the desired N size embedding vector for each of the V words in the vocabulary. This notebook illustrates this concept using fundamental NN calculations.\n",
    "\n",
    "- Skip-gram:\n",
    "\n",
    "Generally similar to CBOW. We pick two nearby words from the training text, we input the one-hot vector of one of them to the NN and try to predict of the softmaxed output is equal to the one-hot of the other one.\n",
    "There are several optimization tricks used such as sub-sampling (to account for the frequency of the words in the corpus) and negative sampling (to account for many non-positive outcomes that dont have to be update). \n",
    "[More info](https://towardsdatascience.com/word2vec-skip-gram-model-part-1-intuition-78614e4d6e0b)\n",
    "\n",
    "- Glove:\n",
    "\n",
    "Glove is based on matrix factorization techniques on the word-context matrix. First a large matrix of (words x context) co-occurrence information, i.e. for each “word” (the rows), you count how frequently we see this word in some “context” (the columns) in a large corpus.  The number of “contexts” is of course large, since it is essentially combinatorial in size.\n",
    "\n",
    "A lower-dimensional (word x features) matrix, where each row now yields a vector representation for the corresponding word is optimzed minimizing a “reconstruction loss”. This loss tries to find the lower-dimensional representations which can explain most of the variance in the high-dimensional data. [More info](https://analyticsindiamag.com/hands-on-guide-to-word-embeddings-using-glove/#:~:text=The%20basic%20idea%20behind%20the,pair%20of%20words%20occurring%20together.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c79f0a",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dfd482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fd4a8",
   "metadata": {},
   "source": [
    "## Reading shakespeare corpus as training dataset\n",
    "\n",
    "Preprocess the data as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84b3e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                                           \n",
    "with open('./data/shakespeare.txt') as f:\n",
    "    data = f.read()                                                 \n",
    "data = re.sub(r'[,!?;-]', '.',data)                                 \n",
    "data = nltk.word_tokenize(data)                                     \n",
    "data = [ ch.lower() for ch in data if ch.isalpha() or ch == '.']    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100e90e5",
   "metadata": {},
   "source": [
    "## Calculate the count of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8191eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 5775 samples and 60976 outcomes>\n",
      "Size of vocabulary:  5775\n",
      "Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1257), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(word for word in data)\n",
    "print(fdist)\n",
    "print(\"Size of vocabulary: \",len(fdist) )\n",
    "print(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b358d",
   "metadata": {},
   "source": [
    "# Create dictionaries\n",
    "\n",
    "Two dictionaries are created to store the index of each word given a word and also return the word at each index given an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf2d8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(data):\n",
    "    word2Ind = {}\n",
    "    Ind2word = {}\n",
    "    counter = 0\n",
    "    fdist = nltk.FreqDist(word for word in data)\n",
    "    for w in fdist.keys():\n",
    "        word2Ind[w] = counter\n",
    "        Ind2word[counter] = w\n",
    "        counter += 1\n",
    "    return word2Ind, Ind2word\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a82235",
   "metadata": {},
   "source": [
    "# Creating the simple NN\n",
    "\n",
    "The NN has one input layer of size V, one hidden layer of size N (the desired size of the word embeddings) and one output layer of size V\n",
    "\n",
    "Usual steps and functions needed for fundamental implementation of NNs. Also available packages like Tensorflow can be used.\n",
    "- Initialization\n",
    "- Forward propagation\n",
    "- Backward propagation\n",
    "- Batch return function\n",
    "- Gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a9fc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V):\n",
    "    W1 = np.random.rand(N, V)\n",
    "    W2 = np.random.rand(V, N)\n",
    "    b1 = np.random.rand(N, 1)\n",
    "    b2 = np.random.rand(V, 1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a43e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    yhat = np.exp(z) / np.sum(np.exp(z), axis = 0)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be87f763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    h = np.matmul(W1, x) + b1\n",
    "    h = np.maximum(h, 0)\n",
    "    \n",
    "    z = np.matmul(W2, h) + b2\n",
    "    return z, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c426d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, yhat, batch_size):\n",
    "    logprobs = np.multiply(np.log(yhat),y)\n",
    "    cost = - 1/batch_size * np.sum(logprobs)\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62d1762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    \n",
    "    l1 = np.matmul(W2.T, yhat - y)\n",
    "    l1 = np.maximum(l1, 0) \n",
    "    \n",
    "    grad_W1 = np.dot(l1, x.T) / batch_size\n",
    "    grad_W2 = np.dot(yhat - y, h.T) / batch_size\n",
    "    \n",
    "    grad_b1 = np.sum(l1, axis = 1, keepdims = True) / batch_size\n",
    "    grad_b2 = np.sum(yhat - y, axis = 1, keepdims = True) / batch_size\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c425135",
   "metadata": {},
   "source": [
    "The function below is implemented as an example. It could be optimized and available packages can also be used.\n",
    "The different hear is that, here, the calculation for x (training batch) and y (label/truth) batch are also done. The calculations use the pre-calculated dictionaries of words vs word-indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db808e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_get_batches(data, word2Ind, V, C, batch_size = 128):\n",
    "    while True:\n",
    "        num_batches = (len(data) - C) // batch_size\n",
    "        for i in range(num_batches - 1):\n",
    "            indices = random.sample(range(C, len(data) - C), 128)\n",
    "            this_batch = []\n",
    "            x = np.zeros((V, batch_size), dtype = float)\n",
    "            y = np.zeros((V, batch_size), dtype = float)\n",
    "            for j in range(batch_size):\n",
    "                start = indices[j]\n",
    "                for k in range(1, C + 1, 1):\n",
    "                    x[word2Ind[data[start - k]]][j] += 1\n",
    "                    x[word2Ind[data[start + k]]][j] += 1\n",
    "                y[word2Ind[data[start]]][j] = 1\n",
    "            x /= (2 * C)\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16dd8f0",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d985ce45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, word2Ind, N, V, num_iters, alpha=0.03, \n",
    "                     random_seed=282, initialize_model=initialize_model, \n",
    "                     get_batches=my_get_batches, forward_prop=forward_prop, \n",
    "                     softmax=softmax, compute_cost=compute_cost, \n",
    "                     back_prop=back_prop):\n",
    "    \n",
    "    W1, W2, b1, b2 = initialize_model(N,V)\n",
    "\n",
    "    batch_size = 128\n",
    "    iters = 0\n",
    "    C = 2 \n",
    "    \n",
    "    for x, y in my_get_batches(data, word2Ind, V, C, batch_size):\n",
    "        \n",
    "        z, h = forward_prop(x, W1, W2, b1, b2)\n",
    "        yhat = softmax(z)\n",
    "        \n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        if ( (iters+1) % 10 == 0):\n",
    "            print(f\"iters: {iters + 1} completed\")\n",
    "            \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = back_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        W1 = W1 - alpha * grad_W1\n",
    "        W2 = W2 - alpha * grad_W2\n",
    "        b1 = b1 - alpha * grad_b1\n",
    "        b2 = b2 - alpha * grad_b2\n",
    "\n",
    "        iters +=1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0711837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 completed\n",
      "iters: 20 completed\n",
      "iters: 30 completed\n",
      "iters: 40 completed\n",
      "iters: 50 completed\n",
      "iters: 60 completed\n",
      "iters: 70 completed\n",
      "iters: 80 completed\n",
      "iters: 90 completed\n",
      "iters: 100 completed\n",
      "iters: 110 completed\n",
      "iters: 120 completed\n",
      "iters: 130 completed\n",
      "iters: 140 completed\n",
      "iters: 150 completed\n"
     ]
    }
   ],
   "source": [
    "C = 2\n",
    "N = 50\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "num_iters = 150\n",
    "print(\"Call gradient_descent\")\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N, V, num_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b62687",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9049dd1",
   "metadata": {},
   "source": [
    "An array of words is created and their embedding is printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4f3fc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding of |king| is [0.19991177 0.78386321 0.65540417 0.50056632 0.68348011 0.61871161\n",
      " 0.33244765 0.36249699 0.47070768 0.55106763 0.21355413 0.43279105\n",
      " 0.45811958 0.19142544 0.16966585 0.1682538  0.689281   0.94125917\n",
      " 0.08626993 0.452611   0.38224913 0.77522007 0.23241242 0.19160534\n",
      " 0.53893683 0.53779821 0.86170196 0.37058287 0.48734814 0.20682348\n",
      " 0.74344852 0.7572905  0.31464416 0.95587407 0.40880683 0.61454012\n",
      " 0.23453226 0.29575051 0.8296693  0.80254727 0.46157702 0.44038062\n",
      " 0.71736285 0.41910073 0.70223745 0.2183107  0.53795768 0.3609458\n",
      " 0.85127576 0.7555842 ]: \n",
      "Embedding of |queen| is [0.39536953 0.78341041 0.30443121 0.02215249 0.54722169 0.19491201\n",
      " 0.91580118 0.46199161 0.42572371 0.35477202 0.21248158 0.59196456\n",
      " 0.51935354 0.03971138 0.63322017 0.40661889 0.59830069 0.41463781\n",
      " 0.556516   0.46224308 0.39296425 0.67493232 0.26148645 0.05587954\n",
      " 0.61918102 0.44264325 0.40712918 0.68841236 0.58539649 0.37318218\n",
      " 0.07064848 0.37751788 0.85233319 0.44384501 0.05667696 0.68591616\n",
      " 0.52896514 0.30019438 0.04959066 0.66716214 0.3577843  0.57867074\n",
      " 0.45577652 0.42958054 0.57709833 0.60413536 0.3312993  0.67861378\n",
      " 0.72994077 0.02531798]: \n",
      "Embedding of |lord| is [0.65429973 0.56664013 0.71631828 0.56165246 0.69368604 0.68461885\n",
      " 0.59508447 0.58029719 0.13400918 0.56378712 0.48753907 0.26356842\n",
      " 0.5723661  0.56739243 0.61382109 0.23452826 0.45827427 0.38210229\n",
      " 0.55095965 0.62090281 0.27563061 0.5473705  0.42103479 0.38544693\n",
      " 0.46340972 0.56032027 0.14279489 0.46593414 0.53756853 0.27827416\n",
      " 0.14210802 0.16570707 0.25034559 0.34302122 0.44845455 0.76525069\n",
      " 0.19464651 0.54536853 0.39485667 0.22670128 0.39379628 0.41262166\n",
      " 0.32881657 0.45868295 0.59478984 0.06813566 0.67994935 0.42241955\n",
      " 0.62532988 0.11566481]: \n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "words = ['king', 'queen','lord','man', 'woman','dog','wolf',\n",
    "         'rich','happy','sad', 'young']\n",
    "\n",
    "embs = (W1.T + W2)/2.0\n",
    " \n",
    "idx = [word2Ind[word] for word in words]\n",
    "X = embs[idx, :]\n",
    "\n",
    "for word in words[:3]:\n",
    "    index = word2Ind[word]\n",
    "    print(\"Embedding of |{}| is {}: \".format(word, embs[index,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea91b14",
   "metadata": {},
   "source": [
    "## Visualizing the word embeddings\n",
    "\n",
    "This is simple model and not optimal. But we can still use unsupervised learning techniques to visualize the dimensionally reduced word vectors.\n",
    "\n",
    "We can implement this using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4adc8bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(2)\n",
    "pca.fit(X)\n",
    "result=  pca.transform(X)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708e4da7",
   "metadata": {},
   "source": [
    "## Probably King is Rich and every man is young! (In shakespeare work at least)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c798755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
