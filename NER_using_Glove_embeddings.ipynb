{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8cdc0d",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) using Glove \n",
    "\n",
    "Named Entity Recognition (NER) has many applications [NER](https://en.wikipedia.org/wiki/Named-entity_recognition) for example in:\n",
    "- Search Engine Efficiency\n",
    "- Recommendation engine\n",
    "- Resume parsing\n",
    "- Customer service\n",
    "\n",
    "Here we used a known dataset from Kaggle at: [Data](https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus)\n",
    "\n",
    "This is the same dataset and general approach as the other notebook (Named Entity Recognition using Keras). The difference is that in the previous notebook the embeddings were learnt as part of the training but here we import [Glove](https://nlp.stanford.edu/data/glove.6B.zip) embeddings to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad23ec",
   "metadata": {},
   "source": [
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da97b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2770b8",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Data has been transferred into txt files and already split into train, validation, and test sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3971352",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/ner_dataset.csv\", encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0efc5eb",
   "metadata": {},
   "source": [
    "## Data Visualization and Preprocessing\n",
    "\n",
    "Checking the data format. The sentences are one after each other and column Sentence # indicates the sentence. We will use that later to break the data into separate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827a8955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag\n",
       "0   Sentence: 1      Thousands  NNS      O\n",
       "1           NaN             of   IN      O\n",
       "2           NaN  demonstrators  NNS      O\n",
       "3           NaN           have  VBP      O\n",
       "4           NaN        marched  VBN      O\n",
       "5           NaN        through   IN      O\n",
       "6           NaN         London  NNP  B-geo\n",
       "7           NaN             to   TO      O\n",
       "8           NaN        protest   VB      O\n",
       "9           NaN            the   DT      O\n",
       "10          NaN            war   NN      O\n",
       "11          NaN             in   IN      O\n",
       "12          NaN           Iraq  NNP  B-geo\n",
       "13          NaN            and   CC      O\n",
       "14          NaN         demand   VB      O\n",
       "15          NaN            the   DT      O\n",
       "16          NaN     withdrawal   NN      O\n",
       "17          NaN             of   IN      O\n",
       "18          NaN        British   JJ  B-gpe\n",
       "19          NaN         troops  NNS      O\n",
       "20          NaN           from   IN      O\n",
       "21          NaN           that   DT      O\n",
       "22          NaN        country   NN      O\n",
       "23          NaN              .    .      O\n",
       "24  Sentence: 2       Families  NNS      O\n",
       "25          NaN             of   IN      O\n",
       "26          NaN       soldiers  NNS      O\n",
       "27          NaN         killed  VBN      O\n",
       "28          NaN             in   IN      O\n",
       "29          NaN            the   DT      O"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0bfd46",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce101e3",
   "metadata": {},
   "source": [
    "For processing the data we need to do a few tasks:\n",
    "- Tokenize each word and each tage in the data to a numerical value. For this we create dictionaries to map each word and tage to a value. Then add columns to the data with corresponding value for the words and tags.\n",
    "- Then we need to separate the sentences. First as we note from the dataframe, the first word of a new sentence has the sentence number. We first fill each Nan with the value of it sentence using [ffil](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html). Then we group the the words belonging to a same sentence together \n",
    "- Then, we use pre-trained Glove word embeddings to map each tokenized word to its embedding.\n",
    "- After all this is done we can split the data into train and plit datasets using sklearn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d2e64e",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "First, we a set from the columns of 'word' and 'tag' from the data. Then we map each word and tag in the data to its numerical value from the dictionaries and add them as new columns to the dataframe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c6d49d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token for id 0 : 1895\n",
      "Tag for id 0:  I-geo\n",
      "Number of unique words : 35178\n",
      "Number of unique tags :  17\n"
     ]
    }
   ],
   "source": [
    "word_set   = list(set(data['Word'].to_list()))\n",
    "word_index = {word:index for  index, word in enumerate(word_set)}\n",
    "index_word = {index:word for  index, word in enumerate(word_set)}\n",
    "\n",
    "tag_set    = list(set(data['Tag'].to_list()))\n",
    "tag_index = {tag:index for  index, tag in enumerate(tag_set)}\n",
    "index_tag = {index:tag for  index, tag in enumerate(tag_set)}\n",
    "\n",
    "\n",
    "print(\"Token for id 0 :\", index_word[3])\n",
    "print(\"Tag for id 0: \", index_tag[4])\n",
    "print(\"Number of unique words : {}\".format(len(word_index)))\n",
    "print(\"Number of unique tags :  {}\".format(len(tag_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e876355",
   "metadata": {},
   "source": [
    "Creating new columns with the mapped numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ccc8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_index</th>\n",
       "      <th>Tag_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>9980</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>32615</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>12071</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>34863</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>29093</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>13810</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>32120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "      <td>29014</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>1524</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>30893</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS    Tag  Word_index  Tag_index\n",
       "0  Sentence: 1      Thousands  NNS      O        9980         13\n",
       "1          NaN             of   IN      O       32615         13\n",
       "2          NaN  demonstrators  NNS      O       12071         13\n",
       "3          NaN           have  VBP      O       34863         13\n",
       "4          NaN        marched  VBN      O       29093         13\n",
       "5          NaN        through   IN      O       13810         13\n",
       "6          NaN         London  NNP  B-geo       32120          0\n",
       "7          NaN             to   TO      O       29014         13\n",
       "8          NaN        protest   VB      O        1524         13\n",
       "9          NaN            the   DT      O       30893         13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Word_index'] = data['Word'].map(word_index)\n",
    "data['Tag_index'] = data['Tag'].map(tag_index)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c2b25b",
   "metadata": {},
   "source": [
    "## Separating the sentences\n",
    "\n",
    "First we with the NaN values with the sentence number for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aeed810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_index</th>\n",
       "      <th>Tag_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>9980</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>32615</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>12071</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>O</td>\n",
       "      <td>34863</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>29093</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>13810</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>32120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "      <td>O</td>\n",
       "      <td>29014</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>protest</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>1524</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>30893</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>war</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>7357</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>10591</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>NNP</td>\n",
       "      <td>B-geo</td>\n",
       "      <td>6806</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>and</td>\n",
       "      <td>CC</td>\n",
       "      <td>O</td>\n",
       "      <td>19166</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demand</td>\n",
       "      <td>VB</td>\n",
       "      <td>O</td>\n",
       "      <td>20282</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>30893</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>withdrawal</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>15735</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>32615</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>British</td>\n",
       "      <td>JJ</td>\n",
       "      <td>B-gpe</td>\n",
       "      <td>2151</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>troops</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>25375</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>from</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>3720</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>that</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>2035</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>country</td>\n",
       "      <td>NN</td>\n",
       "      <td>O</td>\n",
       "      <td>17528</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>20450</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>Families</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>10142</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>32615</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>NNS</td>\n",
       "      <td>O</td>\n",
       "      <td>21773</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>killed</td>\n",
       "      <td>VBN</td>\n",
       "      <td>O</td>\n",
       "      <td>20237</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>in</td>\n",
       "      <td>IN</td>\n",
       "      <td>O</td>\n",
       "      <td>10591</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sentence: 2</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "      <td>O</td>\n",
       "      <td>30893</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sentence #           Word  POS    Tag  Word_index  Tag_index\n",
       "0   Sentence: 1      Thousands  NNS      O        9980         13\n",
       "1   Sentence: 1             of   IN      O       32615         13\n",
       "2   Sentence: 1  demonstrators  NNS      O       12071         13\n",
       "3   Sentence: 1           have  VBP      O       34863         13\n",
       "4   Sentence: 1        marched  VBN      O       29093         13\n",
       "5   Sentence: 1        through   IN      O       13810         13\n",
       "6   Sentence: 1         London  NNP  B-geo       32120          0\n",
       "7   Sentence: 1             to   TO      O       29014         13\n",
       "8   Sentence: 1        protest   VB      O        1524         13\n",
       "9   Sentence: 1            the   DT      O       30893         13\n",
       "10  Sentence: 1            war   NN      O        7357         13\n",
       "11  Sentence: 1             in   IN      O       10591         13\n",
       "12  Sentence: 1           Iraq  NNP  B-geo        6806          0\n",
       "13  Sentence: 1            and   CC      O       19166         13\n",
       "14  Sentence: 1         demand   VB      O       20282         13\n",
       "15  Sentence: 1            the   DT      O       30893         13\n",
       "16  Sentence: 1     withdrawal   NN      O       15735         13\n",
       "17  Sentence: 1             of   IN      O       32615         13\n",
       "18  Sentence: 1        British   JJ  B-gpe        2151         12\n",
       "19  Sentence: 1         troops  NNS      O       25375         13\n",
       "20  Sentence: 1           from   IN      O        3720         13\n",
       "21  Sentence: 1           that   DT      O        2035         13\n",
       "22  Sentence: 1        country   NN      O       17528         13\n",
       "23  Sentence: 1              .    .      O       20450         13\n",
       "24  Sentence: 2       Families  NNS      O       10142         13\n",
       "25  Sentence: 2             of   IN      O       32615         13\n",
       "26  Sentence: 2       soldiers  NNS      O       21773         13\n",
       "27  Sentence: 2         killed  VBN      O       20237         13\n",
       "28  Sentence: 2             in   IN      O       10591         13\n",
       "29  Sentence: 2            the   DT      O       30893         13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filled = data.fillna(method='ffill', axis=0)\n",
    "data_filled.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e14e8d",
   "metadata": {},
   "source": [
    "Then by creating a list of words with the same value 'Sentence #' we group them into a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "887ebe51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Word_index</th>\n",
       "      <th>Tag_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
       "      <td>[O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...</td>\n",
       "      <td>[9980, 32615, 12071, 34863, 29093, 13810, 3212...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 0, 13, 13, 13, 13, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 10</td>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
       "      <td>[B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[28793, 20255, 33376, 13773, 29741, 29014, 316...</td>\n",
       "      <td>[12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 100</td>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
       "      <td>[O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...</td>\n",
       "      <td>[14250, 31241, 325, 18595, 22093, 2854, 10591,...</td>\n",
       "      <td>[13, 13, 5, 13, 13, 13, 13, 13, 0, 13, 13, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1000</td>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[6556, 19800, 31014, 11037, 34349, 28296, 1071...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 10000</td>\n",
       "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
       "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
       "      <td>[B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...</td>\n",
       "      <td>[7468, 17046, 24925, 18233, 8628, 21915, 31096...</td>\n",
       "      <td>[0, 13, 13, 16, 1, 13, 5, 13, 0, 13, 12, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 10001</td>\n",
       "      <td>[Mr., Egeland, said, the, latest, figures, sho...</td>\n",
       "      <td>[NNP, NNP, VBD, DT, JJS, NNS, VBP, CD, CD, NNS...</td>\n",
       "      <td>[B-per, I-per, O, O, O, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[2743, 8628, 21915, 30893, 26524, 11861, 13601...</td>\n",
       "      <td>[16, 1, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 10002</td>\n",
       "      <td>[He, said, last, week, 's, tsunami, and, the, ...</td>\n",
       "      <td>[PRP, VBD, JJ, NN, POS, NN, CC, DT, JJ, NN, NN...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[11451, 21915, 8495, 18535, 34279, 31210, 1916...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sentence: 10003</td>\n",
       "      <td>[Some, 1,27,000, people, are, known, dead, .]</td>\n",
       "      <td>[DT, CD, NNS, VBP, VBN, JJ, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O]</td>\n",
       "      <td>[1656, 16459, 30856, 23491, 5047, 7471, 20450]</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sentence: 10004</td>\n",
       "      <td>[Aid, is, being, rushed, to, the, region, ,, b...</td>\n",
       "      <td>[NNP, VBZ, VBG, VBN, TO, DT, NN, ,, CC, DT, NN...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, B-geo, O, O, O,...</td>\n",
       "      <td>[580, 13033, 29348, 3594, 29014, 30893, 401, 2...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 0, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sentence: 10005</td>\n",
       "      <td>[Lebanese, politicians, are, condemning, Frida...</td>\n",
       "      <td>[JJ, NNS, VBP, VBG, NNP, POS, NN, NN, IN, DT, ...</td>\n",
       "      <td>[B-gpe, O, O, O, B-tim, O, O, O, O, O, O, O, O...</td>\n",
       "      <td>[21627, 14215, 23491, 14003, 6735, 34279, 5223...</td>\n",
       "      <td>[12, 13, 13, 13, 5, 13, 13, 13, 13, 13, 13, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sentence: 10006</td>\n",
       "      <td>[In, Beirut, ,, a, string, of, officials, voic...</td>\n",
       "      <td>[IN, NNP, ,, DT, NN, IN, NNS, VBD, PRP$, NN, ,...</td>\n",
       "      <td>[O, B-geo, O, O, O, O, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[18366, 7787, 26328, 11037, 10409, 32615, 2025...</td>\n",
       "      <td>[13, 0, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Sentence: 10007</td>\n",
       "      <td>[One, person, was, killed, and, more, than, 20...</td>\n",
       "      <td>[CD, NN, VBD, VBN, CC, JJR, IN, CD, NNS, VBN, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[21880, 29862, 25499, 20237, 19166, 13685, 130...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sentence: 10008</td>\n",
       "      <td>[Lebanon, has, suffered, a, series, of, bombin...</td>\n",
       "      <td>[NNP, VBZ, VBN, DT, NN, IN, NNS, IN, DT, JJ, N...</td>\n",
       "      <td>[B-geo, O, O, O, O, O, O, O, O, O, O, O, B-tim...</td>\n",
       "      <td>[33231, 17907, 27414, 11037, 20320, 32615, 253...</td>\n",
       "      <td>[0, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sentence: 10009</td>\n",
       "      <td>[Syria, is, widely, accused, of, involvement, ...</td>\n",
       "      <td>[NNP, VBZ, RB, VBN, IN, NN, IN, PRP$, NN, ,, C...</td>\n",
       "      <td>[B-geo, O, O, O, O, O, O, O, O, O, O, B-tim, O...</td>\n",
       "      <td>[18403, 13033, 8749, 25223, 32615, 30720, 1059...</td>\n",
       "      <td>[0, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 5,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentence: 1001</td>\n",
       "      <td>[The, global, financial, crisis, has, left, Ic...</td>\n",
       "      <td>[DT, JJ, JJ, NN, VBZ, VBN, NNP, POS, NN, IN, N...</td>\n",
       "      <td>[O, O, O, O, O, O, B-org, O, O, O, O, O]</td>\n",
       "      <td>[568, 20780, 1512, 35167, 17907, 19800, 11372,...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 14, 13, 13, 13, 13, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sentence: 10010</td>\n",
       "      <td>[Israeli, officials, say, Prime, Minister, Ari...</td>\n",
       "      <td>[JJ, NNS, VBP, NNP, NNP, NNP, NNP, MD, VB, DT,...</td>\n",
       "      <td>[B-gpe, O, O, B-per, I-per, I-per, I-per, O, O...</td>\n",
       "      <td>[13474, 20255, 33376, 26193, 7565, 31383, 5408...</td>\n",
       "      <td>[12, 13, 13, 16, 1, 1, 1, 13, 13, 13, 13, 13, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Sentence: 10011</td>\n",
       "      <td>[Doctors, describe, the, tiny, hole, as, a, mi...</td>\n",
       "      <td>[NNS, VBP, DT, JJ, NN, IN, DT, JJ, NN, NN, CC,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[27321, 19856, 30893, 19490, 31386, 18478, 110...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sentence: 10012</td>\n",
       "      <td>[The, procedure, ,, known, as, cardiac, cathet...</td>\n",
       "      <td>[DT, NN, ,, VBN, IN, JJ, NN, ,, VBZ, VBG, DT, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[568, 9493, 26328, 5047, 18478, 9541, 7465, 26...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Sentence: 10013</td>\n",
       "      <td>[Doctors, say, they, expect, Mr., Sharon, will...</td>\n",
       "      <td>[NNS, VBP, PRP, VBP, NNP, NNP, MD, VB, DT, JJ,...</td>\n",
       "      <td>[O, O, O, O, B-per, I-per, O, O, O, O, O, O]</td>\n",
       "      <td>[27321, 33376, 13773, 29741, 2743, 5408, 3369,...</td>\n",
       "      <td>[13, 13, 13, 13, 16, 1, 13, 13, 13, 13, 13, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sentence: 10014</td>\n",
       "      <td>[Mr., Sharon, returned, to, work, on, December...</td>\n",
       "      <td>[NNP, NNP, VBD, TO, VB, IN, NNP, CD, ,, CD, NN...</td>\n",
       "      <td>[B-per, I-per, O, O, O, O, B-tim, I-tim, O, B-...</td>\n",
       "      <td>[2743, 5408, 13505, 29014, 19603, 939, 23525, ...</td>\n",
       "      <td>[16, 1, 13, 13, 13, 13, 5, 8, 13, 5, 13, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sentence: 10015</td>\n",
       "      <td>[Doctors, say, the, stroke, has, not, caused, ...</td>\n",
       "      <td>[NNS, VBP, DT, NN, VBZ, RB, VBN, DT, JJ, NN, .]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[27321, 33376, 30893, 4097, 17907, 6214, 9663,...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Sentence: 10016</td>\n",
       "      <td>[The, designers, of, the, first, private, mann...</td>\n",
       "      <td>[DT, NNS, IN, DT, JJ, JJ, VBN, NN, TO, VB, IN,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[568, 34024, 32615, 30893, 4545, 12382, 27276,...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Sentence: 10017</td>\n",
       "      <td>[SpaceShipOne, designer, Burt, Rutan, accepted...</td>\n",
       "      <td>[NNP, NN, NNP, NNP, VBD, DT, NNP, NNP, NNP, NN...</td>\n",
       "      <td>[B-art, O, B-per, I-per, O, O, B-art, I-art, I...</td>\n",
       "      <td>[20752, 2656, 4238, 31422, 31867, 30893, 10949...</td>\n",
       "      <td>[6, 13, 16, 1, 13, 13, 6, 11, 11, 13, 13, 13, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Sentence: 10018</td>\n",
       "      <td>[To, win, the, money, ,, SpaceShipOne, had, to...</td>\n",
       "      <td>[TO, VB, DT, NN, ,, NNP, VBD, TO, VB, RP, IN, ...</td>\n",
       "      <td>[O, O, O, O, O, B-art, O, O, O, O, O, O, O, O,...</td>\n",
       "      <td>[6290, 9727, 30893, 34605, 26328, 20752, 32384...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 6, 13, 13, 13, 13, 13, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sentence: 10019</td>\n",
       "      <td>[The, spacecraft, made, its, flights, in, late...</td>\n",
       "      <td>[DT, NN, VBD, PRP$, NNS, IN, JJ, NNP, CC, JJ, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-tim, O, O, B-tim, O, O...</td>\n",
       "      <td>[568, 8699, 5526, 15564, 5418, 10591, 18372, 1...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 5, 13, 13, 5, 13,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sentence: 1002</td>\n",
       "      <td>[Three, major, banks, have, collapsed, ,, unem...</td>\n",
       "      <td>[CD, JJ, NNS, VBP, VBN, ,, NN, VBZ, VBN, ,, CC...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[20034, 19702, 3085, 34863, 24180, 26328, 2210...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Sentence: 10020</td>\n",
       "      <td>[The, vehicle, had, to, carry, a, pilot, and, ...</td>\n",
       "      <td>[DT, NN, VBD, TO, VB, DT, NN, CC, NN, NN, TO, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[568, 18695, 32384, 29014, 13182, 11037, 22077...</td>\n",
       "      <td>[13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sentence: 10021</td>\n",
       "      <td>[SpaceShipOne, was, financed, with, more, than...</td>\n",
       "      <td>[NNP, VBD, VBN, IN, JJR, IN, $, CD, CD, IN, NN...</td>\n",
       "      <td>[B-art, O, O, O, O, O, O, O, O, O, B-per, I-pe...</td>\n",
       "      <td>[20752, 25499, 5744, 10988, 13685, 13096, 1484...</td>\n",
       "      <td>[6, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Sentence: 10022</td>\n",
       "      <td>[North, Korea, says, flooding, caused, by, las...</td>\n",
       "      <td>[NNP, NNP, VBZ, NN, VBN, IN, JJ, NN, POS, NN, ...</td>\n",
       "      <td>[B-geo, I-geo, O, O, O, O, O, O, O, O, O, B-pe...</td>\n",
       "      <td>[22814, 5087, 14386, 32956, 9663, 19610, 8495,...</td>\n",
       "      <td>[0, 4, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Sentence: 10023</td>\n",
       "      <td>[The, state, news, agency, KCNA, reported, the...</td>\n",
       "      <td>[DT, NN, NN, NN, NNP, VBD, DT, NN, NNP, .]</td>\n",
       "      <td>[O, O, O, O, B-org, O, O, O, B-tim, O]</td>\n",
       "      <td>[568, 27786, 7247, 623, 9274, 27214, 30893, 61...</td>\n",
       "      <td>[13, 13, 13, 13, 14, 13, 13, 13, 5, 13]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentence #                                               Word  \\\n",
       "0       Sentence: 1  [Thousands, of, demonstrators, have, marched, ...   \n",
       "1      Sentence: 10  [Iranian, officials, say, they, expect, to, ge...   \n",
       "2     Sentence: 100  [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "3    Sentence: 1000  [They, left, after, a, tense, hour-long, stand...   \n",
       "4   Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
       "5   Sentence: 10001  [Mr., Egeland, said, the, latest, figures, sho...   \n",
       "6   Sentence: 10002  [He, said, last, week, 's, tsunami, and, the, ...   \n",
       "7   Sentence: 10003      [Some, 1,27,000, people, are, known, dead, .]   \n",
       "8   Sentence: 10004  [Aid, is, being, rushed, to, the, region, ,, b...   \n",
       "9   Sentence: 10005  [Lebanese, politicians, are, condemning, Frida...   \n",
       "10  Sentence: 10006  [In, Beirut, ,, a, string, of, officials, voic...   \n",
       "11  Sentence: 10007  [One, person, was, killed, and, more, than, 20...   \n",
       "12  Sentence: 10008  [Lebanon, has, suffered, a, series, of, bombin...   \n",
       "13  Sentence: 10009  [Syria, is, widely, accused, of, involvement, ...   \n",
       "14   Sentence: 1001  [The, global, financial, crisis, has, left, Ic...   \n",
       "15  Sentence: 10010  [Israeli, officials, say, Prime, Minister, Ari...   \n",
       "16  Sentence: 10011  [Doctors, describe, the, tiny, hole, as, a, mi...   \n",
       "17  Sentence: 10012  [The, procedure, ,, known, as, cardiac, cathet...   \n",
       "18  Sentence: 10013  [Doctors, say, they, expect, Mr., Sharon, will...   \n",
       "19  Sentence: 10014  [Mr., Sharon, returned, to, work, on, December...   \n",
       "20  Sentence: 10015  [Doctors, say, the, stroke, has, not, caused, ...   \n",
       "21  Sentence: 10016  [The, designers, of, the, first, private, mann...   \n",
       "22  Sentence: 10017  [SpaceShipOne, designer, Burt, Rutan, accepted...   \n",
       "23  Sentence: 10018  [To, win, the, money, ,, SpaceShipOne, had, to...   \n",
       "24  Sentence: 10019  [The, spacecraft, made, its, flights, in, late...   \n",
       "25   Sentence: 1002  [Three, major, banks, have, collapsed, ,, unem...   \n",
       "26  Sentence: 10020  [The, vehicle, had, to, carry, a, pilot, and, ...   \n",
       "27  Sentence: 10021  [SpaceShipOne, was, financed, with, more, than...   \n",
       "28  Sentence: 10022  [North, Korea, says, flooding, caused, by, las...   \n",
       "29  Sentence: 10023  [The, state, news, agency, KCNA, reported, the...   \n",
       "\n",
       "                                                  POS  \\\n",
       "0   [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...   \n",
       "1   [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...   \n",
       "2   [NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...   \n",
       "3      [PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]   \n",
       "4   [NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...   \n",
       "5   [NNP, NNP, VBD, DT, JJS, NNS, VBP, CD, CD, NNS...   \n",
       "6   [PRP, VBD, JJ, NN, POS, NN, CC, DT, JJ, NN, NN...   \n",
       "7                      [DT, CD, NNS, VBP, VBN, JJ, .]   \n",
       "8   [NNP, VBZ, VBG, VBN, TO, DT, NN, ,, CC, DT, NN...   \n",
       "9   [JJ, NNS, VBP, VBG, NNP, POS, NN, NN, IN, DT, ...   \n",
       "10  [IN, NNP, ,, DT, NN, IN, NNS, VBD, PRP$, NN, ,...   \n",
       "11  [CD, NN, VBD, VBN, CC, JJR, IN, CD, NNS, VBN, ...   \n",
       "12  [NNP, VBZ, VBN, DT, NN, IN, NNS, IN, DT, JJ, N...   \n",
       "13  [NNP, VBZ, RB, VBN, IN, NN, IN, PRP$, NN, ,, C...   \n",
       "14  [DT, JJ, JJ, NN, VBZ, VBN, NNP, POS, NN, IN, N...   \n",
       "15  [JJ, NNS, VBP, NNP, NNP, NNP, NNP, MD, VB, DT,...   \n",
       "16  [NNS, VBP, DT, JJ, NN, IN, DT, JJ, NN, NN, CC,...   \n",
       "17  [DT, NN, ,, VBN, IN, JJ, NN, ,, VBZ, VBG, DT, ...   \n",
       "18  [NNS, VBP, PRP, VBP, NNP, NNP, MD, VB, DT, JJ,...   \n",
       "19  [NNP, NNP, VBD, TO, VB, IN, NNP, CD, ,, CD, NN...   \n",
       "20    [NNS, VBP, DT, NN, VBZ, RB, VBN, DT, JJ, NN, .]   \n",
       "21  [DT, NNS, IN, DT, JJ, JJ, VBN, NN, TO, VB, IN,...   \n",
       "22  [NNP, NN, NNP, NNP, VBD, DT, NNP, NNP, NNP, NN...   \n",
       "23  [TO, VB, DT, NN, ,, NNP, VBD, TO, VB, RP, IN, ...   \n",
       "24  [DT, NN, VBD, PRP$, NNS, IN, JJ, NNP, CC, JJ, ...   \n",
       "25  [CD, JJ, NNS, VBP, VBN, ,, NN, VBZ, VBN, ,, CC...   \n",
       "26  [DT, NN, VBD, TO, VB, DT, NN, CC, NN, NN, TO, ...   \n",
       "27  [NNP, VBD, VBN, IN, JJR, IN, $, CD, CD, IN, NN...   \n",
       "28  [NNP, NNP, VBZ, NN, VBN, IN, JJ, NN, POS, NN, ...   \n",
       "29         [DT, NN, NN, NN, NNP, VBD, DT, NN, NNP, .]   \n",
       "\n",
       "                                                  Tag  \\\n",
       "0   [O, O, O, O, O, O, B-geo, O, O, O, O, O, B-geo...   \n",
       "1   [B-gpe, O, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "2   [O, O, B-tim, O, O, O, O, O, B-geo, O, O, O, O...   \n",
       "3                   [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "4   [B-geo, O, O, B-per, I-per, O, B-tim, O, B-geo...   \n",
       "5   [B-per, I-per, O, O, O, O, O, O, O, O, O, O, O...   \n",
       "6   [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "7                               [O, O, O, O, O, O, O]   \n",
       "8   [O, O, O, O, O, O, O, O, O, O, B-geo, O, O, O,...   \n",
       "9   [B-gpe, O, O, O, B-tim, O, O, O, O, O, O, O, O...   \n",
       "10  [O, B-geo, O, O, O, O, O, O, O, O, O, O, O, O,...   \n",
       "11  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "12  [B-geo, O, O, O, O, O, O, O, O, O, O, O, B-tim...   \n",
       "13  [B-geo, O, O, O, O, O, O, O, O, O, O, B-tim, O...   \n",
       "14           [O, O, O, O, O, O, B-org, O, O, O, O, O]   \n",
       "15  [B-gpe, O, O, B-per, I-per, I-per, I-per, O, O...   \n",
       "16  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "17  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "18       [O, O, O, O, B-per, I-per, O, O, O, O, O, O]   \n",
       "19  [B-per, I-per, O, O, O, O, B-tim, I-tim, O, B-...   \n",
       "20                  [O, O, O, O, O, O, O, O, O, O, O]   \n",
       "21  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "22  [B-art, O, B-per, I-per, O, O, B-art, I-art, I...   \n",
       "23  [O, O, O, O, O, B-art, O, O, O, O, O, O, O, O,...   \n",
       "24  [O, O, O, O, O, O, O, B-tim, O, O, B-tim, O, O...   \n",
       "25  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "26         [O, O, O, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "27  [B-art, O, O, O, O, O, O, O, O, O, B-per, I-pe...   \n",
       "28  [B-geo, I-geo, O, O, O, O, O, O, O, O, O, B-pe...   \n",
       "29             [O, O, O, O, B-org, O, O, O, B-tim, O]   \n",
       "\n",
       "                                           Word_index  \\\n",
       "0   [9980, 32615, 12071, 34863, 29093, 13810, 3212...   \n",
       "1   [28793, 20255, 33376, 13773, 29741, 29014, 316...   \n",
       "2   [14250, 31241, 325, 18595, 22093, 2854, 10591,...   \n",
       "3   [6556, 19800, 31014, 11037, 34349, 28296, 1071...   \n",
       "4   [7468, 17046, 24925, 18233, 8628, 21915, 31096...   \n",
       "5   [2743, 8628, 21915, 30893, 26524, 11861, 13601...   \n",
       "6   [11451, 21915, 8495, 18535, 34279, 31210, 1916...   \n",
       "7      [1656, 16459, 30856, 23491, 5047, 7471, 20450]   \n",
       "8   [580, 13033, 29348, 3594, 29014, 30893, 401, 2...   \n",
       "9   [21627, 14215, 23491, 14003, 6735, 34279, 5223...   \n",
       "10  [18366, 7787, 26328, 11037, 10409, 32615, 2025...   \n",
       "11  [21880, 29862, 25499, 20237, 19166, 13685, 130...   \n",
       "12  [33231, 17907, 27414, 11037, 20320, 32615, 253...   \n",
       "13  [18403, 13033, 8749, 25223, 32615, 30720, 1059...   \n",
       "14  [568, 20780, 1512, 35167, 17907, 19800, 11372,...   \n",
       "15  [13474, 20255, 33376, 26193, 7565, 31383, 5408...   \n",
       "16  [27321, 19856, 30893, 19490, 31386, 18478, 110...   \n",
       "17  [568, 9493, 26328, 5047, 18478, 9541, 7465, 26...   \n",
       "18  [27321, 33376, 13773, 29741, 2743, 5408, 3369,...   \n",
       "19  [2743, 5408, 13505, 29014, 19603, 939, 23525, ...   \n",
       "20  [27321, 33376, 30893, 4097, 17907, 6214, 9663,...   \n",
       "21  [568, 34024, 32615, 30893, 4545, 12382, 27276,...   \n",
       "22  [20752, 2656, 4238, 31422, 31867, 30893, 10949...   \n",
       "23  [6290, 9727, 30893, 34605, 26328, 20752, 32384...   \n",
       "24  [568, 8699, 5526, 15564, 5418, 10591, 18372, 1...   \n",
       "25  [20034, 19702, 3085, 34863, 24180, 26328, 2210...   \n",
       "26  [568, 18695, 32384, 29014, 13182, 11037, 22077...   \n",
       "27  [20752, 25499, 5744, 10988, 13685, 13096, 1484...   \n",
       "28  [22814, 5087, 14386, 32956, 9663, 19610, 8495,...   \n",
       "29  [568, 27786, 7247, 623, 9274, 27214, 30893, 61...   \n",
       "\n",
       "                                            Tag_index  \n",
       "0   [13, 13, 13, 13, 13, 13, 0, 13, 13, 13, 13, 13...  \n",
       "1   [12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...  \n",
       "2   [13, 13, 5, 13, 13, 13, 13, 13, 0, 13, 13, 13,...  \n",
       "3        [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]  \n",
       "4   [0, 13, 13, 16, 1, 13, 5, 13, 0, 13, 12, 13, 1...  \n",
       "5   [16, 1, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13...  \n",
       "6   [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...  \n",
       "7                        [13, 13, 13, 13, 13, 13, 13]  \n",
       "8   [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 0, 13...  \n",
       "9   [12, 13, 13, 13, 5, 13, 13, 13, 13, 13, 13, 13...  \n",
       "10  [13, 0, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13...  \n",
       "11  [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...  \n",
       "12  [0, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13...  \n",
       "13  [0, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 5,...  \n",
       "14   [13, 13, 13, 13, 13, 13, 14, 13, 13, 13, 13, 13]  \n",
       "15  [12, 13, 13, 16, 1, 1, 1, 13, 13, 13, 13, 13, ...  \n",
       "16  [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...  \n",
       "17  [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...  \n",
       "18    [13, 13, 13, 13, 16, 1, 13, 13, 13, 13, 13, 13]  \n",
       "19  [16, 1, 13, 13, 13, 13, 5, 8, 13, 5, 13, 13, 1...  \n",
       "20       [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]  \n",
       "21  [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...  \n",
       "22  [6, 13, 16, 1, 13, 13, 6, 11, 11, 13, 13, 13, ...  \n",
       "23  [13, 13, 13, 13, 13, 6, 13, 13, 13, 13, 13, 13...  \n",
       "24  [13, 13, 13, 13, 13, 13, 13, 5, 13, 13, 5, 13,...  \n",
       "25  [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...  \n",
       "26  [13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 1...  \n",
       "27  [6, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16, 1,...  \n",
       "28  [0, 4, 13, 13, 13, 13, 13, 13, 13, 13, 13, 16,...  \n",
       "29            [13, 13, 13, 13, 14, 13, 13, 13, 5, 13]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_grouped = data_filled.groupby(['Sentence #'],as_index = False)\n",
    "data_ordered = data_grouped.agg(lambda x: list(x))\n",
    "data_ordered.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a41a21",
   "metadata": {},
   "source": [
    "### Split the data into train, validation, and test\n",
    "\n",
    "Before splitting, we are actually still not finished with pre-processing! We need to make sure that all the tokenized sentences and labels that will go into the model are the same size. This size has to be the size of the maximum sentence that we have found while tokenizing the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284f1d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_padding = len(word_index) - 1\n",
    "sentences  = data_ordered['Word_index'].tolist()\n",
    "tags       = data_ordered['Tag_index'].tolist()\n",
    "maxlen = max([len(s) for s in sentences])\n",
    "\n",
    "sentences_padded = pad_sequences(sentences, maxlen = maxlen, dtype='int32', padding='post', value = words_padding)\n",
    "tags_padded      = pad_sequences(tags     , maxlen = maxlen, dtype='int32', padding='post', value = tag_index['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7247e98d",
   "metadata": {},
   "source": [
    "To be able to calculate cross-entropy loss during the training, we transform each tag to its one hot encoding. Alternatively, we could use categorical cross-entropy if available depending of the tool used for building and training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cca7a2",
   "metadata": {},
   "source": [
    "Now the data is finally ready to be split between train, validation, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9139000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34530, 104)\n",
      "(8633, 104)\n",
      "(4796, 104)\n"
     ]
    }
   ],
   "source": [
    "x_train_val, x_test, y_train_val, y_test = train_test_split(sentences_padded, tags_padded, \n",
    "                                                            test_size = 0.1, train_size = 0.9, random_state = 42)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_val, \n",
    "                                                  y_train_val,test_size = 0.2,train_size = 0.8, random_state = 42)\n",
    "\n",
    "\n",
    "y_train = np.asarray(y_train, dtype = np.int64).squeeze()\n",
    "    \n",
    "y_val = np.asarray(y_val, dtype = np.int64).squeeze()\n",
    "\n",
    "y_test = np.asarray(y_test, dtype = np.int64).squeeze()\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c409ea",
   "metadata": {},
   "source": [
    "## Glove embeddings\n",
    "\n",
    "We want to use Glove embeddings for better performance and using word similarities pre-trained into them. \n",
    "- We first read the file. \n",
    "- Then we create dictionary mapping each word to its embedding.\n",
    "- Then we need to create a matrix that maps each word index into its embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1ac1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(\"./data/glove.6B.100d.txt\", \"r\", encoding=\"utf8\").readlines()\n",
    "\n",
    "word_emd = {}\n",
    "for l in lines:\n",
    "    s = l.split(\" \")\n",
    "    word = s[0]\n",
    "    embedding = np.zeros( (1, len(s)-1))\n",
    "    for k, x in enumerate(s[1:]):\n",
    "        embedding[0,k] = float(x.strip())\n",
    "    word_emd[word] = embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d97f9e",
   "metadata": {},
   "source": [
    "Mapping each word index to its embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "332ea3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(list(word_index.keys()))\n",
    "embed_dim = 100\n",
    "weight_matrix = np.zeros((vocab_length, embed_dim))\n",
    "\n",
    "for i in range(len(list(word_index.keys()))):\n",
    "    try:\n",
    "        word = index_word[i].lower()\n",
    "        weight_matrix[i] = word_emd[word]\n",
    "    except KeyError:\n",
    "        weight_matrix[i] = np.random.normal(scale = 0.6, size=(embed_dim, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27499f59",
   "metadata": {},
   "source": [
    "Creating a torch embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "771c6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_embeddings = weight_matrix.shape[0]\n",
    "weights = torch.from_numpy(weight_matrix)\n",
    "emb_layer = nn.Embedding(num_embeddings, embed_dim)\n",
    "emb_layer.load_state_dict({'weight': weights})\n",
    "emb_layer.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3f9b5",
   "metadata": {},
   "source": [
    "# Setting up the model\n",
    "\n",
    "Embedding layer will be passed to the function that returns the model to create the embedding layer of the model.\n",
    "The model is a LSTM layer followed by a dense layer equal to the size of the labels and the model forward return the softmax probabilities over the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b8fad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, weight_matrix, lstm_hidden_dim, num_of_tags, emb_layer, embed_dim):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        #Embedding layer \n",
    "        self.embedding, embedding_dim = emb_layer, embed_dim\n",
    "\n",
    "        #the LSTM takens embedded sentence\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim, batch_first = True)\n",
    "\n",
    "        #fc layer transforms the output to give the final output layer\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, num_of_tags)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, s):\n",
    "        #apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embedding(s)   # dim: batch_size x batch_max_len x embedding_dim\n",
    "\n",
    "        #run the LSTM along the sentences of length batch_max_len\n",
    "        s, _ = self.lstm(s)     # dim: batch_size x batch_max_len x lstm_hidden_dim                \n",
    "\n",
    "        #reshape the Variable so that each row contains one token\n",
    "        s = s.reshape(-1, s.shape[2])  # dim: batch_size*batch_max_len x lstm_hidden_dim\n",
    "\n",
    "        #apply the fully connected layer and obtain the output for each token\n",
    "        s = self.fc(s)          # dim: batch_size*batch_max_len x num_tags\n",
    "\n",
    "        return F.log_softmax(s, dim = 1)   # dim: batch_size*batch_max_len x num_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a542db",
   "metadata": {},
   "source": [
    "## Defining the loss function\n",
    "\n",
    "Outputs from the model that is sent to the loss function is of shape batch_size x max_len x num_tags and it has probs/softmax values of the tags classes. To calculate the cross-entropy loss we simply need to pick the softmax value corresponding to the true label from the last dimension for all the samples in the batch. To calculate the cost then we average by the total number of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1486e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, batch_y):\n",
    "    #reshape labels to give a flat vector of length batch_size * max_len\n",
    "    batch_y = batch_y.view(-1)  \n",
    "\n",
    "    #the number of tokens is the sum of elements in mask\n",
    "    total_tags = batch_y.shape[0]\n",
    "\n",
    "    #pick the values corresponding to labels and multiply by mask\n",
    "    outputs = outputs[range(outputs.shape[0]), batch_y]\n",
    "\n",
    "    #cross entropy loss for all non 'PAD' tokens\n",
    "    return -torch.sum(outputs) / total_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad62778",
   "metadata": {},
   "source": [
    "## Defing the model and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb088ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net( weight_matrix, lstm_hidden_dim=128, num_of_tags=len(tag_index), emb_layer = emb_layer, embed_dim = embed_dim)\n",
    "\n",
    "optimizer = optim.Adam( model.parameters(), lr = 1e-3 )\n",
    "\n",
    "batch_size=128\n",
    "indices = np.arange(y_train.shape[0])\n",
    "epochs = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7070cd",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We could use the validation set to report the accuracy during the training but it has been skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d92b23eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf35c50ce0c4c4393059c0480bbf399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - 2.685585\n",
      "Epoch: 100 - 0.339507\n",
      "Epoch: 200 - 0.142585\n",
      "Epoch: 300 - 0.114563\n",
      "Epoch: 400 - 0.096106\n",
      "Epoch: 500 - 0.082273\n",
      "Epoch: 600 - 0.073100\n",
      "Epoch: 700 - 0.066558\n",
      "Epoch: 800 - 0.061846\n",
      "Epoch: 900 - 0.057461\n",
      "Epoch: 1000 - 0.054761\n",
      "Epoch: 1100 - 0.052442\n",
      "Epoch: 1200 - 0.050872\n",
      "Epoch: 1300 - 0.048456\n",
      "Epoch: 1400 - 0.047358\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for e in tqdm(range(epochs)):\n",
    "    np.random.shuffle(indices)\n",
    "    batch_indices = indices[:batch_size]\n",
    "    batch_x = x_train[batch_indices]\n",
    "    batch_y = y_train[batch_indices]\n",
    "\n",
    "    batch_x = torch.from_numpy(batch_x)\n",
    "    batch_y = torch.from_numpy(batch_y)\n",
    "\n",
    "    probs = model.forward(batch_x)\n",
    "    loss = loss_fn(probs, batch_y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.detach())\n",
    "    if e % 100 == 0:\n",
    "        print(\"Epoch: %d - %.6f\" %(e, np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c924279",
   "metadata": {},
   "source": [
    "# Evaluating on the validation dataset\n",
    "\n",
    "For each of the tages we calculate precision, recall, and f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccfbf6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: B-geo \t- Precision: 0.7753 - Recall: 0.8185 - f1: 0.7963 - Support: 3797.0000\n",
      "Label: I-per \t- Precision: 0.7974 - Recall: 0.8806 - f1: 0.8369 - Support: 1658.0000\n",
      "Label: I-nat \t- Precision: 0.0000 - Recall: 0.0000 - f1: 0.0000 - Support: 7.0000\n",
      "Label: I-gpe \t- Precision: 0.0000 - Recall: 0.0000 - f1: 0.0000 - Support: 16.0000\n",
      "Label: I-geo \t- Precision: 0.6892 - Recall: 0.6003 - f1: 0.6417 - Support: 713.0000\n",
      "Label: B-tim \t- Precision: 0.8950 - Recall: 0.7000 - f1: 0.7855 - Support: 2033.0000\n",
      "Label: B-art \t- Precision: 0.0000 - Recall: 0.0000 - f1: 0.0000 - Support: 46.0000\n",
      "Label: I-org \t- Precision: 0.6991 - Recall: 0.3378 - f1: 0.4556 - Support: 1699.0000\n",
      "Label: I-tim \t- Precision: 0.8322 - Recall: 0.4068 - f1: 0.5465 - Support: 585.0000\n",
      "Label: B-nat \t- Precision: 0.0000 - Recall: 0.0000 - f1: 0.0000 - Support: 20.0000\n",
      "Label: I-eve \t- Precision: 0.0000 - Recall: 0.0000 - f1: 0.0000 - Support: 39.0000\n",
      "Label: I-art \t- Precision: 0.0000 - Recall: 0.0000 - f1: 0.0000 - Support: 40.0000\n",
      "Label: B-gpe \t- Precision: 0.8868 - Recall: 0.8662 - f1: 0.8764 - Support: 1592.0000\n",
      "Label: O \t- Precision: 0.9928 - Recall: 0.9982 - f1: 0.9955 - Support: 482781.0000\n",
      "Label: B-org \t- Precision: 0.6472 - Recall: 0.3187 - f1: 0.4271 - Support: 2055.0000\n",
      "Label: B-eve \t- Precision: 0.0000 - Recall: 0.0000 - f1: 0.0000 - Support: 35.0000\n",
      "Label: B-per \t- Precision: 0.7529 - Recall: 0.7308 - f1: 0.7417 - Support: 1668.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "batch_size = 128\n",
    "k = 0\n",
    "\n",
    "preds = None\n",
    "while k < x_test.shape[0]:\n",
    "    x = x_test[k:k+batch_size] if k+batch_size < x_test.shape[0] else x_test[k:]\n",
    "    y = y_test[k:k+batch_size] if k+batch_size < y_test.shape[0] else y_test[k:]\n",
    "\n",
    "    x = torch.from_numpy(x)\n",
    "    y = torch.from_numpy(y)\n",
    "\n",
    "    probs = model.forward(x).detach().numpy()\n",
    "    yhat = np.argmax(probs, axis=1)\n",
    "\n",
    "    preds = yhat if preds is None else np.hstack( (preds, yhat) )\n",
    "\n",
    "    k += batch_size\n",
    "\n",
    "labels = [ index_tag[i] for i in range(len(index_tag)) ]\n",
    "y_test = y_test.reshape((-1,))\n",
    "\n",
    "p, r, f, s = precision_recall_fscore_support( y_test, preds, zero_division = 0)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    print(\"Label: %s \\t- Precision: %.4f - Recall: %.4f - f1: %.4f - Support: %.4f\" %(labels[i], p[i], r[i], f[i], s[i]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771ccf1",
   "metadata": {},
   "source": [
    "We can note that the tags that had very small number of occurance have zero accuracy. Sometimes we may not care as they are small percentage of the data. But if their occurance is critical and has a lot of information that cannot be missed we need to do oversampling. \n",
    "\n",
    "This can be implemented using a sampler in a dataloader. An example of that is shown in the notebook for probability calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8e17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32ec3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
