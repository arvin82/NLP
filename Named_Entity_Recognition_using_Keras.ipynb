{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8cdc0d",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) using Keras\n",
    "\n",
    "Named Entity Recognition (NER) has many applications [NER](https://en.wikipedia.org/wiki/Named-entity_recognition) for example in:\n",
    "- Search Engine Efficiency\n",
    "- Recommendation engine\n",
    "- Resume parsing\n",
    "- Customer service\n",
    "\n",
    "Here we used a known dataset from Kaggle at: [Data](https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ad23ec",
   "metadata": {},
   "source": [
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da97b59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rnd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import tensorflow.keras.layers as tfl\n",
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Embedding,Dense\n",
    "from tensorflow.keras import Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense\n",
    "from tensorflow.keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2770b8",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Data has been transferred into txt files and already split into train, validation, and test sub-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c1ed38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train data\n",
    "with open(\"./data/train_sentences.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    t_sentences = f.readlines()\n",
    "\n",
    "with open(\"./data/train_labels.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    t_labels = f.readlines()\n",
    "\n",
    "#Validation data\n",
    "with open(\"./data/eval_sentences.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    v_sentences = f.readlines()\n",
    "\n",
    "with open(\"./data/eval_labels.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    v_labels = f.readlines()\n",
    "\n",
    "#Test data\n",
    "with open(\"./data/test_sentences.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    test_sentences = f.readlines()\n",
    "\n",
    "with open(\"./data/test_labels.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    test_labels = f.readlines()\n",
    "    \n",
    "#Tags\n",
    "with open(\"./data/tags.txt\", 'r', encoding=\"utf8\") as f:\n",
    "    tags = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0efc5eb",
   "metadata": {},
   "source": [
    "## Data Visualization and Preprocessing\n",
    "\n",
    "Checking the first sentence in training data, Looking both at the sentence and the tags for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b7b5859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .\n",
      "\n",
      "O O O O O O B-geo O O O O O B-geo O O O O O B-gpe O O O O O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(t_sentences[0])\n",
    "print(t_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0bfd46",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182edc0f",
   "metadata": {},
   "source": [
    "For processing the data we need to do a few tasks:\n",
    "- Tokenize each sentence (in all train, validation, and test datasets)\n",
    "- Tokenize the tag labels (Here, we have to make our own dictionary)\n",
    "- Pad all sentences and labels to the maximum size\n",
    "\n",
    "For tokenizing the sentences, we can use pre-trained models with good word embeddings, but here we to test, we don't do this and we will create a dictionary of the words and assign each word to a number as we encounter that while going through all the words in the corpus.\n",
    "\n",
    "The number of classes is small and we only need to go through our training dataset to find all of them. We create a dictionary to translate each tag to a number.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c6d49d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = {}\n",
    "tag_index  = {}\n",
    "\n",
    "counter = 0\n",
    "for sentence in t_sentences:\n",
    "    sentence = sentence.strip('\\n')\n",
    "    sentence = sentence.split(' ')\n",
    "    for word in sentence:\n",
    "        if word not in word_index:\n",
    "            word_index[word] = counter + 1\n",
    "            counter += 1\n",
    "for sentence in v_sentences:\n",
    "    sentence = sentence.strip('\\n')\n",
    "    sentence = sentence.split(' ')\n",
    "    for word in sentence:\n",
    "        if word not in word_index:\n",
    "            word_index[word] = counter + 1\n",
    "            counter += 1\n",
    "for sentence in test_sentences:\n",
    "    sentence = sentence.strip('\\n')\n",
    "    sentence = sentence.split(' ')\n",
    "    for word in sentence:\n",
    "        if word not in word_index:\n",
    "            word_index[word] = counter + 1\n",
    "            counter += 1\n",
    "            \n",
    "word_index['UNK'] = counter\n",
    "            \n",
    "counter = 0\n",
    "for tags in t_labels:\n",
    "    tags = tags.strip('\\n')\n",
    "    tags = tags.split(' ')\n",
    "    for tag in tags:\n",
    "        if tag not in tag_index:\n",
    "            tag_index[tag] = counter\n",
    "            counter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c02251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words : 35179\n",
      "Number of unique tags :  17\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words : {}\".format(len(word_index)))\n",
    "print(\"Number of unique tags :  {}\".format(len(tag_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a41a21",
   "metadata": {},
   "source": [
    "### Tokenizing the sentences using the words dictionary\n",
    "\n",
    "We create a function and pass train, validation, and test sentences to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d96b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ad3ff71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences, word_index, max_len = 0):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip('\\n')\n",
    "        sentence = sentence.split(' ')\n",
    "        max_len = max(max_len, len(sentence))\n",
    "        tokenized_sentence = []\n",
    "        for word in sentence:\n",
    "            tokenized_sentence.append(word_index[word])\n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "    return tokenized_sentences, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01706b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "x_train, max_len = tokenize_sentences(t_sentences, word_index, 0)\n",
    "x_val, max_len = tokenize_sentences(v_sentences, word_index, max_len)\n",
    "x_test, max_len = tokenize_sentences(test_sentences, word_index, max_len)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886f0cd",
   "metadata": {},
   "source": [
    "## Tokenizing the tags using the tags dictionary\n",
    "We create a function and pass train, validation, and test labels to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5776a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tag(tags_sequences, tag_index):\n",
    "    encoded_tags_sequences = []\n",
    "    for sequence in tags_sequences:\n",
    "        sequence = sequence.strip('\\n')\n",
    "        sequence = sequence.split(' ')\n",
    "        tags = []\n",
    "        for tag in sequence:\n",
    "            tags.append(tag_index[tag])\n",
    "        encoded_tags_sequences.append(tags)\n",
    "    return encoded_tags_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2302ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = encode_tag(t_labels, tag_index)\n",
    "y_val   = encode_tag(v_labels, tag_index)\n",
    "y_test  = encode_tag(test_labels, tag_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e68da95",
   "metadata": {},
   "source": [
    "### Visualizing the data so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10065d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 10, 16, 2, 17, 18, 19, 20, 21, 22]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0]\n",
      "35179\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "Vocab_size = len(word_index)\n",
    "\n",
    "print(Vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d9f57",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "We are still not finished with pre-processing. We need to make sure that all the tokenized sentences and labels that will go into the model are the same size. This size has to be the size of the maximum sentence that we have found while tokenizing the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "936a670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "x_train_padded = pad_sequences(maxlen=max_len, sequences=x_train, padding=\"post\", value=0)\n",
    "y_train_padded = pad_sequences(maxlen=max_len, sequences=y_train, padding=\"post\", value=tag_index['O'])\n",
    "\n",
    "x_val_padded = pad_sequences(maxlen=max_len, sequences=x_val, padding=\"post\", value=0)\n",
    "y_val_padded = pad_sequences(maxlen=max_len, sequences=y_val, padding=\"post\", value=tag_index['O'])\n",
    "\n",
    "x_test_padded = pad_sequences(maxlen=max_len, sequences=x_test, padding=\"post\", value=0)\n",
    "y_test_padded = pad_sequences(maxlen=max_len, sequences=y_test, padding=\"post\", value=tag_index['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612baa6",
   "metadata": {},
   "source": [
    "## Visualizing the pre-processed data\n",
    "\n",
    "OK, now we can see that the sentences are tokenized into numbers for each word and labels are also encoded based on their tag_index hash.\n",
    "\n",
    "Processing the input data is usually the most time-consuming part. And now we are ready to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "711ddbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 10 16  2 17 18 19 20 21 22\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "[0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_padded[0])\n",
    "print(y_train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed61d7a",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "The model is an embedding layer, followed by one LSTM, and then a dense layer with number of layers equal to our number of unique tags.\n",
    "\n",
    "We could also use to_categorical to go from the token code to a one_hot vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5644030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 50)            1758950   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 50, 50)           0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50, 100)           60400     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50, 17)            1717      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,821,067\n",
      "Trainable params: 1,821,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "        [\n",
    "            tfl.Input(shape = (max_len, )),\n",
    "            tfl.Embedding(input_dim  = Vocab_size, output_dim = 50, input_length = max_len),\n",
    "            tfl.SpatialDropout1D(0.1),\n",
    "            tfl.LSTM(units = 100, return_sequences=True, recurrent_dropout=0.1),\n",
    "            tfl.Dense(len(tag_index), activation = 'softmax'),\n",
    "        ])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bc995",
   "metadata": {},
   "source": [
    "### Setting the optimizer\n",
    "\n",
    "The key thing here is to use \"sparse_categoritcal_crossentropy\" because the targets are number of each class and not their hot_vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4702e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49204abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1050/1050 [==============================] - 31s 28ms/step - loss: 0.2462 - accuracy: 0.9476 - val_loss: 0.1214 - val_accuracy: 0.9666\n",
      "Epoch 2/3\n",
      "1050/1050 [==============================] - 30s 29ms/step - loss: 0.0844 - accuracy: 0.9779 - val_loss: 0.0687 - val_accuracy: 0.9806\n",
      "Epoch 3/3\n",
      "1050/1050 [==============================] - 30s 28ms/step - loss: 0.0536 - accuracy: 0.9843 - val_loss: 0.0613 - val_accuracy: 0.9815\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=x_train_padded,\n",
    "    y=y_train_padded,\n",
    "    validation_data=(x_val_padded,y_val_padded),\n",
    "    batch_size=32, \n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e80ae",
   "metadata": {},
   "source": [
    "## Evaluating on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "238f7f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225/225 [==============================] - 1s 5ms/step - loss: 0.0613 - accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.061315711587667465, 0.9815123677253723]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_val_padded, y_val_padded, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be53b49",
   "metadata": {},
   "source": [
    "## Evaluating on the custom input\n",
    "\n",
    "We may encounter unknown words when processing a custom sentence and and an unknown token was added to the word dictionary when it was created that can be used here. But here we simply apply the pad token to unknown words to see what happens!\n",
    "\n",
    "This step is important because one trivial convergence point of the NN could be that it will classifity all data as 'O' becuase there are many 'O' because of padding compared to other tags. Because of this we also had to reduce max_len from real value of 114 to 50 to have less padding. The distribution of sentences with more than 50 length is much smaller and adding more padding forces the NN to converge to 'O'. There could be other techniques to improve this but this is a simplification for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1f180cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_custom_sentences(sentences, word_index, max_len = 0):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip('\\n')\n",
    "        sentence = sentence.strip('.')\n",
    "        sentence = sentence.split(' ')\n",
    "        max_len = max(max_len, len(sentence))\n",
    "        tokenized_sentence = []\n",
    "        for word in sentence:\n",
    "            if word in word_index:\n",
    "                tokenized_sentence.append(word_index[word])\n",
    "            else:\n",
    "                tokenized_sentence.append(Vocab_size - 1)\n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "    return tokenized_sentences, max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aaaeae",
   "metadata": {},
   "source": [
    "### Tokenize and pad the input sentence\n",
    "\n",
    "The custom sentence has to be processed the same as the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "848641fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sentence = \"Monday morning we are going to have a meeting with Peter to asasdfdfd about San Diego traffic\"\n",
    "custom_sentence_processed = custom_sentence.strip('.\\n').split(' ')\n",
    "inputs = []\n",
    "inputs.append(custom_sentence)\n",
    "tokenized_input, _ = tokenize_custom_sentences(inputs, word_index)\n",
    "padded_tokenized_input = pad_sequences(maxlen=max_len, sequences=tokenized_input, padding=\"post\", value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01353687",
   "metadata": {},
   "source": [
    "### Run the processed sentence through the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc43a2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 164ms/step\n"
     ]
    }
   ],
   "source": [
    "output_custom = model.predict(padded_tokenized_input)\n",
    "custom = output_custom[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b704c1",
   "metadata": {},
   "source": [
    "### Extract the tagged words and visualize the result\n",
    "\n",
    "First we need to create the inverse dictionary of tag_index.\n",
    "Then we need to find the max class for each word using argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76b60945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday\t\tB-tim\n",
      "morning\t\tI-tim\n",
      "Peter\t\tB-per\n",
      "San\t\tB-geo\n",
      "Diego\t\tI-geo\n"
     ]
    }
   ],
   "source": [
    "tags = list(tag_index.keys())\n",
    "output_decoded = np.argmax(custom, axis = 1)\n",
    "output_decoded = np.argmax(custom, axis = 1)\n",
    "\n",
    "for i in range(len(output_decoded)):\n",
    "    if output_decoded[i] != 0:\n",
    "        print(custom_sentence_processed[i] + '\\t\\t' + tags[output_decoded[i]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab32ec3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
