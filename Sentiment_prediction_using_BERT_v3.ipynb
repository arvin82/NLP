{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Learning using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Reference and guide: [Ref1](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#41-bertforsequenceclassification) and [Ref2](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT introduction\n",
    "\n",
    "BERT is a large-scale transformer-based Language Model that can be finetuned for a variety of tasks.\n",
    "\n",
    "The original paper can be found [here](https://arxiv.org/abs/1810.04805). \n",
    "\n",
    "[HuggingFace documentation](https://huggingface.co/transformers/model_doc/bert.html)\n",
    "\n",
    "[Bert documentation](https://characters.fandom.com/wiki/Bert_(Sesame_Street) ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMILE Twitter dataset: [Link](https://figshare.com/articles/dataset/smile_annotations_final_csv/3187909/2)\n",
    "\n",
    "Not the best dataset for this test but the same concept can be used on other datasets.\n",
    "Another notebook will use the same approach for imdb rating prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame = pd.read_csv('Data/smile-annotations-final.csv', names = ['id', 'text', 'label'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the begging of the dataset that is loaded in the form of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>611857364396965889</td>\n",
       "      <td>@aandraous @britishmuseum @AndrewsAntonio Merc...</td>\n",
       "      <td>nocode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>614484565059596288</td>\n",
       "      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>614746522043973632</td>\n",
       "      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>614877582664835073</td>\n",
       "      <td>@Sofabsports thank you for following me back. ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>611932373039644672</td>\n",
       "      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  611857364396965889  @aandraous @britishmuseum @AndrewsAntonio Merc...   \n",
       "1  614484565059596288  Dorian Gray with Rainbow Scarf #LoveWins (from...   \n",
       "2  614746522043973632  @SelectShowcase @Tate_StIves ... Replace with ...   \n",
       "3  614877582664835073  @Sofabsports thank you for following me back. ...   \n",
       "4  611932373039644672  @britishmuseum @TudorHistory What a beautiful ...   \n",
       "\n",
       "    label  \n",
       "0  nocode  \n",
       "1   happy  \n",
       "2   happy  \n",
       "3   happy  \n",
       "4   happy  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the id column for index and checking again the format of the raw dataframe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>611857364396965889</th>\n",
       "      <td>@aandraous @britishmuseum @AndrewsAntonio Merc...</td>\n",
       "      <td>nocode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614484565059596288</th>\n",
       "      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614746522043973632</th>\n",
       "      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614877582664835073</th>\n",
       "      <td>@Sofabsports thank you for following me back. ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611932373039644672</th>\n",
       "      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 text   label\n",
       "id                                                                           \n",
       "611857364396965889  @aandraous @britishmuseum @AndrewsAntonio Merc...  nocode\n",
       "614484565059596288  Dorian Gray with Rainbow Scarf #LoveWins (from...   happy\n",
       "614746522043973632  @SelectShowcase @Tate_StIves ... Replace with ...   happy\n",
       "614877582664835073  @Sofabsports thank you for following me back. ...   happy\n",
       "611932373039644672  @britishmuseum @TudorHistory What a beautiful ...   happy"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame.set_index('id', inplace = True)\n",
    "DataFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking all the categories in the raw data\n",
    "\n",
    "We can see that the data is not really balanced and may not be so good for training. As the number of samples in some categories is really small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nocode               1572\n",
       "happy                1137\n",
       "not-relevant          214\n",
       "angry                  57\n",
       "surprise               35\n",
       "sad                    32\n",
       "happy|surprise         11\n",
       "happy|sad               9\n",
       "disgust|angry           7\n",
       "disgust                 6\n",
       "sad|disgust             2\n",
       "sad|angry               2\n",
       "sad|disgust|angry       1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the columns that have not been labeled (nocode). \n",
    "Creating a dictionary for the labels and assigning a number to each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'happy': 0,\n",
       " 'not-relevant': 1,\n",
       " 'angry': 2,\n",
       " 'disgust': 3,\n",
       " 'sad': 4,\n",
       " 'surprise': 5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame = DataFrame[~DataFrame.label.str.contains('\\|')]\n",
    "DataFrame = DataFrame[DataFrame.label != 'nocode']\n",
    "DataFrame.label.value_counts()\n",
    "unique_labels = DataFrame.label.unique()\n",
    "dict = {}\n",
    "for i, label in enumerate(unique_labels):\n",
    "    dict[label] = i\n",
    "dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column, named new label, in the dataframe with the value of assigned category for each tweet (row). And then we check the dataframe again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp/ipykernel_32164/1165857530.py:6: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  DataFrame = DataFrame.drop('label', 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>new_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>614484565059596288</th>\n",
       "      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614746522043973632</th>\n",
       "      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614877582664835073</th>\n",
       "      <td>@Sofabsports thank you for following me back. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611932373039644672</th>\n",
       "      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611570404268883969</th>\n",
       "      <td>@NationalGallery @ThePoldarkian I have always ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614499696015503361</th>\n",
       "      <td>Lucky @FitzMuseum_UK! Good luck @MirandaStearn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613601881441570816</th>\n",
       "      <td>Yr 9 art students are off to the @britishmuseu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613696526297210880</th>\n",
       "      <td>@RAMMuseum Please vote for us as @sainsbury #s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610746718641102848</th>\n",
       "      <td>#AskTheGallery Have you got plans to privatise...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612648200588038144</th>\n",
       "      <td>@BarbyWT @britishmuseum so beautiful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 text  \\\n",
       "id                                                                      \n",
       "614484565059596288  Dorian Gray with Rainbow Scarf #LoveWins (from...   \n",
       "614746522043973632  @SelectShowcase @Tate_StIves ... Replace with ...   \n",
       "614877582664835073  @Sofabsports thank you for following me back. ...   \n",
       "611932373039644672  @britishmuseum @TudorHistory What a beautiful ...   \n",
       "611570404268883969  @NationalGallery @ThePoldarkian I have always ...   \n",
       "614499696015503361  Lucky @FitzMuseum_UK! Good luck @MirandaStearn...   \n",
       "613601881441570816  Yr 9 art students are off to the @britishmuseu...   \n",
       "613696526297210880  @RAMMuseum Please vote for us as @sainsbury #s...   \n",
       "610746718641102848  #AskTheGallery Have you got plans to privatise...   \n",
       "612648200588038144               @BarbyWT @britishmuseum so beautiful   \n",
       "\n",
       "                    new_label  \n",
       "id                             \n",
       "614484565059596288          0  \n",
       "614746522043973632          0  \n",
       "614877582664835073          0  \n",
       "611932373039644672          0  \n",
       "611570404268883969          0  \n",
       "614499696015503361          0  \n",
       "613601881441570816          0  \n",
       "613696526297210880          1  \n",
       "610746718641102848          1  \n",
       "612648200588038144          0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_label(row):\n",
    "    old_label = row['label']\n",
    "    return dict[old_label]\n",
    "\n",
    "DataFrame['new_label'] = DataFrame.apply(lambda row : apply_label(row), axis=1)\n",
    "DataFrame = DataFrame.drop('label', 1)\n",
    "\n",
    "DataFrame.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use train_test_split from sklearn.\n",
    "Based on the result from train_test_split, we again modify our dataframe by adding a column data_type and check the value based on its assignment to train or test sets. Not that this step is random, and also our final model metrics will depend on this (because our input data is not so uniformly distributed between categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>new_label</th>\n",
       "      <th>data_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>614484565059596288</th>\n",
       "      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614746522043973632</th>\n",
       "      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614877582664835073</th>\n",
       "      <td>@Sofabsports thank you for following me back. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611932373039644672</th>\n",
       "      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611570404268883969</th>\n",
       "      <td>@NationalGallery @ThePoldarkian I have always ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614499696015503361</th>\n",
       "      <td>Lucky @FitzMuseum_UK! Good luck @MirandaStearn...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613601881441570816</th>\n",
       "      <td>Yr 9 art students are off to the @britishmuseu...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613696526297210880</th>\n",
       "      <td>@RAMMuseum Please vote for us as @sainsbury #s...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610746718641102848</th>\n",
       "      <td>#AskTheGallery Have you got plans to privatise...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612648200588038144</th>\n",
       "      <td>@BarbyWT @britishmuseum so beautiful</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                 text  \\\n",
       "id                                                                      \n",
       "614484565059596288  Dorian Gray with Rainbow Scarf #LoveWins (from...   \n",
       "614746522043973632  @SelectShowcase @Tate_StIves ... Replace with ...   \n",
       "614877582664835073  @Sofabsports thank you for following me back. ...   \n",
       "611932373039644672  @britishmuseum @TudorHistory What a beautiful ...   \n",
       "611570404268883969  @NationalGallery @ThePoldarkian I have always ...   \n",
       "614499696015503361  Lucky @FitzMuseum_UK! Good luck @MirandaStearn...   \n",
       "613601881441570816  Yr 9 art students are off to the @britishmuseu...   \n",
       "613696526297210880  @RAMMuseum Please vote for us as @sainsbury #s...   \n",
       "610746718641102848  #AskTheGallery Have you got plans to privatise...   \n",
       "612648200588038144               @BarbyWT @britishmuseum so beautiful   \n",
       "\n",
       "                    new_label data_type  \n",
       "id                                       \n",
       "614484565059596288          0     train  \n",
       "614746522043973632          0     train  \n",
       "614877582664835073          0     train  \n",
       "611932373039644672          0     train  \n",
       "611570404268883969          0     train  \n",
       "614499696015503361          0     train  \n",
       "613601881441570816          0     train  \n",
       "613696526297210880          1     train  \n",
       "610746718641102848          1     train  \n",
       "612648200588038144          0     train  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(DataFrame.index.values,\n",
    "                                                    DataFrame.new_label,\n",
    "                                                    test_size = 0.15,\n",
    "                                                    random_state = 17,\n",
    "                                                    stratify = DataFrame.new_label.values)\n",
    "\n",
    "DataFrame['data_type'] = ['not_set'] * DataFrame.shape[0]\n",
    "DataFrame.loc[X_train, 'data_type'] = 'train'\n",
    "DataFrame.loc[X_test, 'data_type'] = 'test'\n",
    "DataFrame.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can check how much from each category is assigned to train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new_label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
       "      <th>test</th>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
       "      <th>test</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
       "      <th>test</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
       "      <th>test</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
       "      <th>test</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">5</th>\n",
       "      <th>test</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     text\n",
       "new_label data_type      \n",
       "0         test        171\n",
       "          train       966\n",
       "1         test         32\n",
       "          train       182\n",
       "2         test          9\n",
       "          train        48\n",
       "3         test          1\n",
       "          train         5\n",
       "4         test          5\n",
       "          train        27\n",
       "5         test          5\n",
       "          train        30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame.groupby(['new_label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data\n",
    "\n",
    "We try BertTokenizer from BERT and load a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking some random sequences of words. It seems 101 refers to start of a sentence and 102 refers to an end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2572, 2188, 102], [101, 2073, 2024, 2017, 1029, 102]], 'token_type_ids': [[0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer.batch_encode_plus(['this is the first sentence', 'another setence'])\n",
    "tokenizer.batch_encode_plus(['I am home', 'Where are you?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checking that it works, we use this tokenizer to process/tokenize our dataframe, and using the new column of data_type, we create tokenized train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "d:\\Users\\HP\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2211: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenizer.batch_encode_plus(\n",
    "    DataFrame[DataFrame.data_type == 'train'].text.values,\n",
    "    add_special_tokens = True, return_attention_mask = True,\n",
    "    pad_to_max_length = True,\n",
    "    max_length = 256,\n",
    "    return_tensors = 'pt')\n",
    "\n",
    "tokenized_test = tokenizer.batch_encode_plus(\n",
    "    DataFrame[DataFrame.data_type == 'test'].text.values,\n",
    "    add_special_tokens = True, return_attention_mask = True,\n",
    "    pad_to_max_length = True,\n",
    "    max_length = 256,\n",
    "    return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tokenized datasets, we extract and store ids, attention masks (all 1 at the moment), and the labels. \n",
    "Then, we create TensorDatasets from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = tokenized_train['input_ids']\n",
    "attention_mask_train = tokenized_train['attention_mask']\n",
    "labels_train = torch.tensor(\n",
    "    DataFrame[DataFrame.data_type == 'train'].new_label.values)\n",
    "\n",
    "input_ids_test = tokenized_test['input_ids']\n",
    "attention_mask_test = tokenized_test['attention_mask']\n",
    "labels_test = torch.tensor(\n",
    "    DataFrame[DataFrame.data_type == 'test'].new_label.values)\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_mask_train, labels_train)\n",
    "dataset_test  = TensorDataset(input_ids_test , attention_mask_test , labels_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up BERT Pretrained Model\n",
    "\n",
    "Since this is a sentence (sequence) to multi-class classification, we load the pre-trained Sequence Classification model: https://huggingface.co/docs/transformers/model_doc/bert\n",
    "\n",
    "The Huggingface list other available models that can be used for other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = len(dict), output_attentions = False,\n",
    "                                      output_hidden_states = False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (6, 768)\n",
      "classifier.bias                                                 (6,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Data Loaders\n",
    "\n",
    "When working with batches, we always have to worry about dataloaders. Sometimes, we can write our own dataloader, but there are also available methods in libraries. Here, we use it from torch.utils.data\n",
    "\n",
    "Here, we have to decide on the size of the batches that we want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4 #32\n",
    "train_dataloader = DataLoader(dataset_train, sampler = RandomSampler(dataset_train), batch_size = batch_size)\n",
    "test_dataloader =  DataLoader(dataset_test,  sampler = RandomSampler(dataset_test ), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the optimizer and scheduler\n",
    "\n",
    "We choose adam optimized and set some of the hyper-parameters like learning rate and epsilon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 1e-5, eps = 1e-8)\n",
    "epochs = 5\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, \n",
    "                                            num_training_steps = len(train_dataloader) * epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Performance Metrics\n",
    "\n",
    "Define a F1_score_func function and use f1_score from sklearn.\n",
    "\n",
    "$$\n",
    "    F_{1} = \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    Precision = \\frac{TruePositive}{TruePositive + FalsePositive}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    Recall = \\frac{TruePositive}{TruePositive + FalseNegative}\n",
    "$$\n",
    "\n",
    "Precisions will tell us what percentage of all positives are actually possitive.\n",
    "Recall will tell us what percentage of all actual positives were correctly predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "    preds_flat  = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average = 'weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to calculate F1_score for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "    preds_flat  = np.argmax (preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    inverse_dict = {v:k for k, v in dict.items()}\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true  = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {inverse_dict[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach adapted from an older version of HuggingFace's `run_glue.py` script. Accessible [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed_val = 17\n",
    "#random.seed(seed_val)\n",
    "#np.random.seed(seed_val)\n",
    "#torch.manual_seed(seed_val)\n",
    "#torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an evaluate function. tqdm is used to visualize the progress during evaluation and training.\n",
    "Explanation on how the model is called and parameters: [Link1](https://discuss.huggingface.co/t/new-model-output-types/195) [Link2](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128)\n",
    "\n",
    "Note: **inputs sends the reference to key, value of inputs to the model\n",
    "\n",
    "First evaluation function in created and then a training loop is created with the same principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader_val):\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38ec626023d4a92a248778daabf49df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training Loss: 0.7482757588937169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730d71f6990c408799fa2ba8272f20cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.7389112422575376\n",
      "F1 score: 0.7306529543722665\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Training Loss: 0.4725364663135556\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16809b1068824635ac6b421a5c70165e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.597670856505699\n",
      "F1 score: 0.7758654900098663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Training Loss: 0.3480159034494251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60cfe0e58ee4876946b4ff3e6e9382c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6773433836767383\n",
      "F1 score: 0.8406402477927365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Training Loss: 0.24191346026602245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4945866b8d43488a802eb9b147b2af7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6586198837467236\n",
      "F1 score: 0.8419474617655489\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Training Loss: 0.18654966705981346\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096dd2c3e75b4207b9c1a297d9b71d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.6560705022961234\n",
      "F1 score: 0.8441678668900734\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    loss_train_total = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc='Epoch {:1d}'.format(epoch),\n",
    "                       leave = False, disable = False)\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids'       : batch[0],\n",
    "                  'attention_mask'   : batch[1],\n",
    "                  'labels'           : batch[2]\n",
    "                 }\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({f'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "    \n",
    "    #model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    torch.save(model.state_dict(), f'Models/BERT_ft_epoch{epoch}.model')\n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    loss_train_avg = loss_train_total / len(train_dataloader)\n",
    "    tqdm.write(f'Training Loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(test_dataloader)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 score: {val_f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our the Model\n",
    "\n",
    "Now, we apply the model to the test dateset and use our accuracy per class function to output the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0479ee325fa3406eb29e30316c4e389c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_, predictions, true_values = evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: happy\n",
      "Accuracy: 160/171\n",
      "Class: not-relevant\n",
      "Accuracy: 21/32\n",
      "Class: angry\n",
      "Accuracy: 8/9\n",
      "Class: disgust\n",
      "Accuracy: 0/1\n",
      "Class: sad\n",
      "Accuracy: 0/5\n",
      "Class: surprise\n",
      "Accuracy: 2/5\n"
     ]
    }
   ],
   "source": [
    "accuracy_per_class(predictions, true_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on custom input\n",
    "\n",
    "The cool thing is that, now we can input any sentence to the model and see how it is categorizing them into the classes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_dict = {v:k for k, v in dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input sentence:I do not want to see you\t Prediction is: angry\n"
     ]
    }
   ],
   "source": [
    "my_tweet = \"I do not want to see you\"\n",
    "inputs = tokenizer(my_tweet, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "labels.to(device)\n",
    "\n",
    "with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "\n",
    "logits = outputs[\"logits\"]\n",
    "logits = logits.detach().cpu().numpy()\n",
    "category  = np.argmax(logits, axis = 1).flatten()\n",
    "\n",
    "print(\"For input sentence:\" + my_tweet + \"\\t Prediction is: \" + inverse_dict[category[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input sentence:Why is it not raining today!\t Prediction is: surprise\n"
     ]
    }
   ],
   "source": [
    "my_tweet = \"Why is it not raining today!\"\n",
    "inputs = tokenizer(my_tweet, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "labels.to(device)\n",
    "\n",
    "with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "\n",
    "logits = outputs[\"logits\"]\n",
    "logits = logits.detach().cpu().numpy()\n",
    "category  = np.argmax(logits, axis = 1).flatten()\n",
    "\n",
    "print(\"For input sentence:\" + my_tweet + \"\\t Prediction is: \" + inverse_dict[category[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For input sentence:I am feeling much better today. Thank you.\t Prediction is: happy\n"
     ]
    }
   ],
   "source": [
    "my_tweet = \"I am feeling much better today. Thank you.\"\n",
    "inputs = tokenizer(my_tweet, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "labels.to(device)\n",
    "\n",
    "with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "\n",
    "logits = outputs[\"logits\"]\n",
    "logits = logits.detach().cpu().numpy()\n",
    "category  = np.argmax(logits, axis = 1).flatten()\n",
    "\n",
    "print(\"For input sentence:\" + my_tweet + \"\\t Prediction is: \" + inverse_dict[category[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
