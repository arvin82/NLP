{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f54f6b92",
   "metadata": {},
   "source": [
    "# Parts-of-Speech Tagging (POS)\n",
    "\n",
    "This is an example of POS: [Link](https://www.freecodecamp.org/news/an-introduction-to-part-of-speech-tagging-and-the-hidden-markov-model-953d45338f24/#:~:text=HMMs%20for%20Part%20of%20Speech%20Tagging&text=The%20states%20in%20an%20HMM,POS%20tags%20for%20the%20words.)\n",
    "\n",
    "Training data: WSJ-2_21.pos\n",
    "Test data: WSJ-24.pos\n",
    "\n",
    "Information about the tags used in the dataset: [Tags](http://relearn.be/2015/training-common-sense/sources/software/pattern-2.6-critical-fork/docs/html/mbsp-tags.html)\n",
    "\n",
    "In this notebook two approaches are used. First, simply by calculating transition and emission materices of Hidden Markov Model (HMM) and assigning the highest probability at each location/word of the text.\n",
    "\n",
    "The second approach uses Viterbi algorithm to consider the previous word's tag and probability when assigning the tag for each word.\n",
    "\n",
    "There are many other techniques and models like LSTMs and BERT that have been used and reported in the literature for the same datasets. [Link1](https://paperswithcode.com/task/part-of-speech-tagging) [Link2](http://nlpprogress.com/english/part-of-speech_tagging.html#:~:text=A%20standard%20dataset%20for%20POS,are%20evaluated%20based%20on%20accuracy.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd7a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47100c74",
   "metadata": {},
   "source": [
    "## Reading and visualizing the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c00cf169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data\n",
    "with open(\"./data/WSJ_02-21.pos\", 'r') as f:\n",
    "    training_corpus = f.readlines()\n",
    "\n",
    "#Vocabulary already created from the training data\n",
    "with open(\"./data/hmm_vocab.txt\", 'r') as f:\n",
    "    voc_l = f.read().split('\\n')\n",
    "\n",
    "#Test data\n",
    "with open(\"./data/WSJ_24.pos\", 'r') as f:\n",
    "    y = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb34e1ff",
   "metadata": {},
   "source": [
    "Creating a vocabulary dictionary from the words. This will be later used to give the index of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275e7f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "for i, word in enumerate(sorted(voc_l)): \n",
    "    vocab[word] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b993b8c",
   "metadata": {},
   "source": [
    "Some preprocessing to improve detecting unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b9bbe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "punct = set(string.punctuation)\n",
    "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "adv_suffix = [\"ward\", \"wards\", \"wise\"]\n",
    "\n",
    "def assign_unk(tok):\n",
    "    # Digits\n",
    "    if any(char.isdigit() for char in tok):\n",
    "        return \"--unk_digit--\"\n",
    "\n",
    "    # Punctuation\n",
    "    elif any(char in punct for char in tok):\n",
    "        return \"--unk_punct--\"\n",
    "\n",
    "    # Upper-case\n",
    "    elif any(char.isupper() for char in tok):\n",
    "        return \"--unk_upper--\"\n",
    "\n",
    "    # Nouns\n",
    "    elif any(tok.endswith(suffix) for suffix in noun_suffix):\n",
    "        return \"--unk_noun--\"\n",
    "\n",
    "    # Verbs\n",
    "    elif any(tok.endswith(suffix) for suffix in verb_suffix):\n",
    "        return \"--unk_verb--\"\n",
    "\n",
    "    # Adjectives\n",
    "    elif any(tok.endswith(suffix) for suffix in adj_suffix):\n",
    "        return \"--unk_adj--\"\n",
    "\n",
    "    # Adverbs\n",
    "    elif any(tok.endswith(suffix) for suffix in adv_suffix):\n",
    "        return \"--unk_adv--\"\n",
    "\n",
    "    return \"--unk--\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf92e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag(word_tag, vocab):\n",
    "    if not word_tag.split():\n",
    "        word = \"--n--\"\n",
    "        tag = \"--s--\"\n",
    "        return word, tag\n",
    "    else:\n",
    "        word, tag = word_tag.split()\n",
    "        if word not in vocab: \n",
    "            \n",
    "            word = assign_unk(word)\n",
    "        return word, tag\n",
    "    return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b924c14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(vocab, y):\n",
    "    words = []\n",
    "    for line in y:\n",
    "        word, tag = get_word_tag(line, vocab)\n",
    "        words.append(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c225342",
   "metadata": {},
   "source": [
    "Processing the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "264d0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep = preprocess(vocab, y)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c29655",
   "metadata": {},
   "source": [
    "# Create Transition and Emission matrices\n",
    "\n",
    "Transition matrix dictionary gives the number of seen transitions between two tags. Each time the two tags happen after each other we increment the value of the key = (tag1, tag2)\n",
    "\n",
    "Emission matrix dictionary gives the number of times a word is seen with a tag. Each time the word with a certain tag is seen we increment the value of the key = (tag, word)\n",
    "\n",
    "These matrices are again be used later for creating the Hidden Markov Model matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46d09fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_corpus, vocab, verbose=True):\n",
    "    \n",
    "    emission_counts = defaultdict(int)\n",
    "    transition_counts = defaultdict(int)\n",
    "    tag_counts = defaultdict(int)\n",
    "    \n",
    "    prev_tag = '--s--' \n",
    "    \n",
    "    for word_tag in training_corpus:\n",
    "        \n",
    "        word, tag = get_word_tag(word_tag, vocab)\n",
    "        \n",
    "        transition_counts[(prev_tag, tag)] += 1\n",
    "        \n",
    "        emission_counts[(tag, word)] += 1\n",
    "\n",
    "        tag_counts[tag] += 1\n",
    "\n",
    "        prev_tag = tag\n",
    "        \n",
    "    return emission_counts, transition_counts, tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a81a3a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_counts, transition_counts, tag_counts = create_dictionaries(training_corpus, vocab)\n",
    "states = sorted(tag_counts.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d33293",
   "metadata": {},
   "source": [
    "# Predictions with approach 1:\n",
    "\n",
    "We simply check the emission matrix and for each word return the tag that has the maximal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5f5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pos(prep, y, emission_counts, vocab, states):\n",
    "    \n",
    "    num_correct = 0\n",
    "    \n",
    "    all_words = set(emission_counts.keys())\n",
    "    \n",
    "    total = len(y)\n",
    "    for word, y_tup in zip(prep, y): \n",
    "\n",
    "        y_tup_l = y_tup.split()\n",
    "        \n",
    "        if len(y_tup_l) == 2:\n",
    "            \n",
    "            true_label = y_tup_l[1]\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        count_final = 0\n",
    "        pos_final = ''\n",
    "        \n",
    "        if word in vocab:\n",
    "            for pos in states:\n",
    "\n",
    "                key = (pos, word)\n",
    "\n",
    "                if key in emission_counts:\n",
    "\n",
    "                    count = emission_counts.get(key)\n",
    "\n",
    "                    if count > count_final:\n",
    "\n",
    "                        count_final = count\n",
    "\n",
    "                        pos_final = pos\n",
    "\n",
    "            if pos_final == true_label: \n",
    "                num_correct += 1\n",
    "            \n",
    "    accuracy = num_correct / total\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c0acbf",
   "metadata": {},
   "source": [
    "### Reporting the accuracy of approach 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d95d328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of prediction using predict_pos is 0.8914\n"
     ]
    }
   ],
   "source": [
    "accuracy_predict_pos = predict_pos(prep, y, emission_counts, vocab, states)\n",
    "print(f\"Accuracy of prediction using predict_pos is {accuracy_predict_pos:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebff273c",
   "metadata": {},
   "source": [
    "# Hidden Markov Model:\n",
    "\n",
    "Two matrices need to be created from the training data:\n",
    "- Transition Matrix: A(i, j) = probability of state j right after state i\n",
    "- Emission Matrix: B(i, j) = probaility of the word j happening at state i\n",
    "\n",
    "First these two matrices are calculated and then Viterbi algorithm is used to find the best probability and tag for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ab361",
   "metadata": {},
   "source": [
    "## Creating the transition and emission matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2a31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
    "    \n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    num_tags = len(all_tags)\n",
    "    \n",
    "    A = np.zeros((num_tags,num_tags))\n",
    "    \n",
    "    trans_keys = set(transition_counts.keys())\n",
    "    \n",
    "    for i in range(num_tags):\n",
    "        \n",
    "        count_prev_tag = tag_counts.get(all_tags[i])\n",
    "        \n",
    "        for j in range(num_tags):\n",
    "            count = 0\n",
    "        \n",
    "            key = (all_tags[i], all_tags[j]) \n",
    "\n",
    "            if key in transition_counts: \n",
    "                \n",
    "                count = transition_counts.get(key)                \n",
    "            A[i,j] = (count + alpha) / (count_prev_tag + num_tags * alpha)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01c3ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
    "    \n",
    "    num_tags = len(tag_counts)\n",
    "    \n",
    "    all_tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    num_words = len(vocab)\n",
    "    \n",
    "    B = np.zeros((num_tags, num_words))\n",
    "    \n",
    "    emis_keys = set(list(emission_counts.keys()))\n",
    "    \n",
    "    for i in range(num_tags): \n",
    "        \n",
    "        for j in range(num_words): \n",
    "\n",
    "            count = 0 \n",
    "                    \n",
    "            key = (all_tags[i], vocab[j]) \n",
    "\n",
    "            if key in emission_counts: \n",
    "        \n",
    "                count = emission_counts.get(key)\n",
    "                \n",
    "            count_tag = tag_counts.get(all_tags[i])\n",
    "                \n",
    "            B[i,j] = (count + alpha) / (count_tag + num_words * alpha)\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2239b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "A = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
    "B = create_emission_matrix(alpha, tag_counts, emission_counts, list(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd643a",
   "metadata": {},
   "source": [
    "## Viterbi algorithm\n",
    "\n",
    "Viterbi algorithm ([Link](https://en.wikipedia.org/wiki/Viterbi_algorithm)) is a dynamic programming algorithm for calculating the maximum a posteriori probability estimate of the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events.\n",
    "\n",
    "The main idea is that, we start from the beginning of the sentence. The sentence is started by a start tag and thus initialized that way. \n",
    "\n",
    "For each word, we look at all possible tags and fill the matrix C so that:\n",
    "C(i, j) = max( C(k, j - 1) * A(k, i) * B (i, index(word[j])) ) over all possible k (tags)\n",
    "\n",
    "In the above:\n",
    "- C(k, j - 1) is the max probability for having k as the tag for previous work\n",
    "- A(k, j) is the transition probability from tag k to tag j\n",
    "- B(i, index(word[j])) is the emission probability of having word[i] with tag i\n",
    "\n",
    "This will be done for all possible tags for the word[j] and j is started from 0 in the begging of the sentence to the end length of the sentence. Then the maximum value and the index of the tag that had the highest probability are stored.\n",
    "\n",
    "D(i, j) = the tag index at which C(:, j) is maximum\n",
    "\n",
    "The algorithm has three steps:\n",
    "- Initialization: the first column of the C, and D matrices are filled since we are always going to look at one step before.\n",
    "- Forward Pass: At each column (meaning each word), we use the already calculated values from the previous column, and using the equations above calculate the values of the column. And find the highest probability tag.\n",
    "- Backward Pass: When all the columns are found, we move backward from the last word in the sentence to the begining and find the tag for each word based on the highest probability of the column of tags for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "322d4060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(states, tag_counts, A, B, corpus, vocab):\n",
    "    \n",
    "    num_tags = len(states)\n",
    "    \n",
    "    best_probs = np.zeros((num_tags, len(corpus)))\n",
    "    \n",
    "    best_paths = np.zeros((num_tags, len(corpus)), dtype=int)\n",
    "    \n",
    "    s_idx = states.index(\"--s--\")\n",
    "    for i in range(num_tags): \n",
    "        \n",
    "        if A[s_idx, i] == 0: \n",
    "            \n",
    "            best_probs[i,0] = float(\"-inf\")\n",
    "            \n",
    "        else:\n",
    "            best_probs[i,0] = math.log(A[s_idx, i]) + math.log(B[i][vocab[corpus[0]]])\n",
    "    \n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a524085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_forward(A, B, test_corpus, best_probs, best_paths, vocab, verbose=True):\n",
    "    \n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    for i in range(1, len(test_corpus)): \n",
    "        \n",
    "        if i % 10000 == 0 and verbose:\n",
    "            print(\"Words processed: {:>8}\".format(i))\n",
    "            \n",
    "        for j in range(num_tags): \n",
    "            \n",
    "            best_prob_i = float(\"-inf\")\n",
    "            \n",
    "            best_path_i = None \n",
    "\n",
    "            for k in range(num_tags): \n",
    "            \n",
    "                prob = best_probs[k, i - 1] + math.log(A[k, j]) + math.log(B[j, vocab[test_corpus[i]]])\n",
    "\n",
    "                if prob > best_prob_i: \n",
    "                    \n",
    "                    best_prob_i = prob\n",
    "                    best_path_i = k\n",
    "\n",
    "            best_probs[j,i] = best_prob_i\n",
    "            \n",
    "            best_paths[j,i] = best_path_i\n",
    "\n",
    "    return best_probs, best_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1689690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_backward(best_probs, best_paths, corpus, states):\n",
    "    \n",
    "    m = best_paths.shape[1] \n",
    "    \n",
    "    z = [None] * m\n",
    "    \n",
    "    num_tags = best_probs.shape[0]\n",
    "    \n",
    "    best_prob_for_last_word = float('-inf')\n",
    "    \n",
    "    pred = [None] * m\n",
    "    l = len(corpus)\n",
    "    \n",
    "    for k in range(num_tags): \n",
    "\n",
    "        if best_probs[k, m - 1] > best_prob_for_last_word: \n",
    "            \n",
    "            best_prob_for_last_word = best_probs[k, m - 1]\n",
    "\n",
    "            z[m - 1] = k\n",
    "            \n",
    "    pred[m - 1] = states[k]\n",
    "    \n",
    "    for i in range(l - 1, -1, -1): \n",
    "        pos_tag_for_word_i = z[i]\n",
    "        z[i - 1] = best_paths[pos_tag_for_word_i, i]\n",
    "        pred[i - 1] = states[z[i - 1]]\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77dd2541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words processed:    10000\n",
      "Words processed:    20000\n",
      "Words processed:    30000\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep, vocab)\n",
    "\n",
    "# Forward pass\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep, best_probs, best_paths, vocab)\n",
    "\n",
    "# Backward pass and find the tags\n",
    "pred = viterbi_backward(best_probs, best_paths, prep, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0275d64",
   "metadata": {},
   "source": [
    "## Calculate the accuracy\n",
    "\n",
    "We can compare the estimate tag of each word with the labeled true value in the test data and calculate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8ce98d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred, y):\n",
    "    \n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for prediction, y in zip(pred, y):\n",
    "        word_tag_tuple = y.split()\n",
    "        \n",
    "        if len(word_tag_tuple) < 2: \n",
    "            continue\n",
    "\n",
    "        word, tag = word_tag_tuple\n",
    "        \n",
    "        if tag == prediction: \n",
    "            num_correct += 1\n",
    "            \n",
    "        total += 1\n",
    "\n",
    "    return num_correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33b72114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Viterbi algorithm is 0.9545\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy of the Viterbi algorithm is {compute_accuracy(pred, y):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7c891e",
   "metadata": {},
   "source": [
    "## We can see that the accuracy is improve from 89% to 95.5%\n",
    "\n",
    "This is great, because we can see from the literature that much more complicated models based on deep-NNs give accuracies in the range of 97-98%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a83831",
   "metadata": {},
   "source": [
    "# Tagging an arbitrary and untagged sentence:\n",
    "\n",
    "The above had processing for a tagged training and test data. What if we want to apply the model on any sentence that is not tagged?\n",
    "\n",
    "We can write a simple function to create data in the same format. In this case, accuracy cannot be tested and we are only interested in the tags of the tested model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d111c1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process_sentence(sentence):\n",
    "    words = re.findall( r'\\w+|[^\\s\\w]+', sentence)\n",
    "    prep_ = []\n",
    "    for word in words:\n",
    "        if word not in vocab: \n",
    "            word = assign_unk(word)\n",
    "        prep_.append(word)\n",
    "    return prep_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c5111",
   "metadata": {},
   "source": [
    "You can change the sentence string variable to your desired text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f187c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"how are you today?\"\n",
    "sentence = sentence.lower()\n",
    "prep_sentence = process_sentence(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4becedd",
   "metadata": {},
   "source": [
    "Applying the tested model to the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "566b0728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "best_probs, best_paths = initialize(states, tag_counts, A, B, prep_sentence, vocab)\n",
    "\n",
    "# Forward pass\n",
    "best_probs, best_paths = viterbi_forward(A, B, prep_sentence, best_probs, best_paths, vocab)\n",
    "\n",
    "# Backward pass and find the tags\n",
    "pred = viterbi_backward(best_probs, best_paths, prep_sentence, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d6e353",
   "metadata": {},
   "source": [
    "## Checking the predicted tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19b5365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how:WRB\n",
      "are:VBP\n",
      "you:PRP\n",
      "today:NN\n",
      "?:#\n"
     ]
    }
   ],
   "source": [
    "for word, tag in zip(prep_sentence, pred):\n",
    "    print(word + \":\" + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75999298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
