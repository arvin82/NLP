{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668edac9",
   "metadata": {},
   "source": [
    "# N-gram Language Models\n",
    "\n",
    "In this notebook an n-gram language model is created and its application for sentence auto-complete is shown.\n",
    "\n",
    "N-gram language models [Link](https://en.wikipedia.org/wiki/N-gram) have applications in auto-complete, speech recognition, spelling correction, and augmative communications.\n",
    "\n",
    "N-gram is a probabilistic language model. Meaning that the probability of the existance of a sequence of n words is calculated from the training corpus. \n",
    "\n",
    "The probability of existance of a word after a n-word sequence P(word | n-word sequence) is equal to number of time 'n-sequence' + word is seen in the corpus, divided by number of time n-word sequence is encountered in the sequence. To do the calculations quickly hashtables with key word of all n-gram and (n+1)-gram have to be created and stored in the memory. This notedbook shows how this can be done efficiently. To account for the cases when the sequences are not seen in the training data a tick, called smoothing, is used to have low probability instead of divisions by zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a277349",
   "metadata": {},
   "source": [
    "## Loading necessary libraries\n",
    "nltk is used for tokenizing the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad82c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ed72a",
   "metadata": {},
   "source": [
    "## Loading and visualizing the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eed2358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'str'>\n",
      "Number of letters: 3335477\n",
      "First 300 letters of the data\n",
      "-------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n"
     ]
    }
   ],
   "source": [
    "filename = \"./data/en_US.twitter.txt\"\n",
    "\n",
    "file = open(filename, encoding=\"utf8\")\n",
    "\n",
    "data = file.read()\n",
    "\n",
    "print(\"Data type:\", type(data))\n",
    "print(\"Number of letters:\", len(data))\n",
    "print(\"First 300 letters of the data\")\n",
    "print(\"-------\")\n",
    "display(data[0:300])\n",
    "print(\"-------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ba921",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "Data preprecessing includes:\n",
    "- Split the corpus into train and test datasets (e.g. 80% train and 20% test)\n",
    "- split the data into sentences.\n",
    "- tokenizing each sentence\n",
    "- Removing words that are not so frequent (rare words. e.g. we may want to only consider words appearing at least twice)\n",
    "- Replacing unknown words with an unknown token\n",
    "\n",
    "\n",
    "For each of these a function is created and the corpus is passed to be be splitted into sentences and tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51a23b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    sentences = data.split('\\n')\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    return sentences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e19afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences: # complete this line\n",
    "        sentence = sentence.lower()\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69529fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_data(data):\n",
    "    sentences = split_to_sentences(data)\n",
    "    tokenized_sentences = tokenize_sentences(sentences)\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4cfaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = get_tokenized_data(data)\n",
    "random.shuffle(tokenized_data)\n",
    "train_size = int(len(tokenized_data) * 0.8)\n",
    "train_data = tokenized_data[0:train_size]\n",
    "test_data = tokenized_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dc2b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokenized_sentences):\n",
    "    word_counts = {}\n",
    "    for sentence in tokenized_sentences:\n",
    "        for token in sentence: \n",
    "            if token not in word_counts: \n",
    "                word_counts[token] = 1\n",
    "            else:\n",
    "                word_counts[token] += 1\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb09d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    closed_vocab = []\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    closed_vocab = [word for word, cnt in word_counts.items() if cnt >=  count_threshold]\n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef8b0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    vocabulary = set(vocabulary)\n",
    "    replaced_tokenized_sentences = []\n",
    "    for sentence in tokenized_sentences:\n",
    "        replaced_sentence = []\n",
    "        for token in sentence: \n",
    "            if token in vocabulary: \n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac1347b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold, unknown_token=\"<unk>\", get_words_with_nplus_frequency=get_words_with_nplus_frequency, replace_oov_words_by_unk=replace_oov_words_by_unk):\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token)\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token)\n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aedb5aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_freq = 2\n",
    "train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n",
    "                                                                        test_data, \n",
    "                                                                        minimum_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71768dcd",
   "metadata": {},
   "source": [
    "## Creating n-gram language model\n",
    "\n",
    "The base of the language model is counting the number of time an n-gram and an (n+1)-gram is seen. Then the probability of the occurance is calculated by the ratio as mentioned above.\n",
    "\n",
    "A function to calculate a count dictionary for all seen n-grams in the data is created.\n",
    "\n",
    "Two other functions are created to calculate the probabilities and store them in the memory. A smoothing trick is used so that division by zero in case of unseen n-grams or (n+1)-grams is replaced by a small number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ade6409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    n_grams = {}\n",
    "    \n",
    "    for sentence in data: \n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "        sentence = tuple(sentence)\n",
    "        \n",
    "        for i in range(len(sentence) - n + 1): \n",
    "            n_gram = sentence[i:i + n]\n",
    "            if n_gram in n_grams: \n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "                \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eae9f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probability(word, previous_n_gram, \n",
    "                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
    "    denominator = previous_n_gram_count + k * vocabulary_size\n",
    "    n_plus1_gram = previous_n_gram + (word, )\n",
    "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
    "    numerator = n_plus1_gram_count + k\n",
    "    probability = numerator / denominator\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaade85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\",  k=1.0):\n",
    "    previous_n_gram = tuple(previous_n_gram)    \n",
    "    vocabulary = vocabulary + [end_token, unknown_token]    \n",
    "    vocabulary_size = len(vocabulary)    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           vocabulary_size, k=k)\n",
    "        probabilities[word] = probability\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f321a4",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "For training the model for an n-gram model, count tables for n-grams and (n+1)-grams need to be calculated.\n",
    "To test the model for different values of n, we calculate the n-grams for n = 1 to 5. Then we have the n-gram language models for n = 1 to n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd82c4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing n-gram counts with n = 1 ...\n",
      "Computing n-gram counts with n = 2 ...\n",
      "Computing n-gram counts with n = 3 ...\n",
      "Computing n-gram counts with n = 4 ...\n",
      "Computing n-gram counts with n = 5 ...\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts_list = []\n",
    "n_max = 4\n",
    "for n in range(1, n_max + 2):\n",
    "    print(\"Computing n-gram counts with n =\", n, \"...\")\n",
    "    n_model_counts = count_n_grams(train_data_processed, n)\n",
    "    n_gram_counts_list.append(n_model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ca1ffe",
   "metadata": {},
   "source": [
    "## Auto complete\n",
    "\n",
    "Using the calculated n-grams, we can make suggestions for the next word.\n",
    "\n",
    "The functions below do this by iterating over all the words in the vocabulary and calculate the probabilities for each, given the previous sequences and return the highest probability word(s).\n",
    "\n",
    "The first function receives two n-gram count tables and returns the most likely word based on that model and the second function is defined to pass two consequative n-gram tables to the first one and receive a suggested model based on that n-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c933120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0, start_with=None):\n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    suggestion = None\n",
    "    max_prob = 0\n",
    "    for word, prob in probabilities.items(): \n",
    "        if start_with is not None: \n",
    "            if not word.startswith(start_with): \n",
    "                continue\n",
    "        if prob > max_prob: \n",
    "            suggestion = word\n",
    "            max_prob = prob\n",
    "    return suggestion, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27e79eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "        \n",
    "        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, start_with=start_with)\n",
    "        suggestions.append(suggestion)\n",
    "    return suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c37f8ad",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138fc08",
   "metadata": {},
   "source": [
    "Now, we can use these functions to get suggestions for a next word from the previous sequence (an uncompleted sentence).\n",
    "\n",
    "The suggestions are shown for different n-grams, meaning looking at n previous words in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36830af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you']\n",
      "The suggestions are:\n",
      "1-gram model: (\"'re\", 0.02392422634114477)\n",
      "2-gram model: ('?', 0.0025874079479864657)\n",
      "3-gram model: ('?', 0.0014236322961155175)\n",
      "4-gram model: ('<e>', 0.0001360451669954425)\n"
     ]
    }
   ],
   "source": [
    "my_sentence = \"hey how are you\"\n",
    "my_sentence_tokenized = get_tokenized_data(my_sentence)[0]\n",
    "\n",
    "suggestions = get_suggestions(my_sentence_tokenized, n_gram_counts_list, vocabulary, k=1.0)\n",
    "\n",
    "print(\"The previous words are {}\".format(my_sentence_tokenized))\n",
    "print(\"The suggestions are:\")\n",
    "for i in range(1, n_max + 1, 1):\n",
    "    print(\"{}-gram model: {}\".format(i, suggestions[i - 1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ed09fb",
   "metadata": {},
   "source": [
    "Here, it is interesting to see that 1-gram suggests 're. It is only looking at the previous work and the most frequent word after you appears to be 're to result in you are. But higher numbers of n predict differently as they look at more words back in the sequence.\n",
    "\n",
    "It is also possible to suggest/return word that start with a certain characters. This is very useful for example to suggest words as the user maybe still typing and has entered part of the next word...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a708cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The previous words are ['hey', 'how', 'are', 'you']\n",
      "The suggestions are:\n",
      "1-gram model: ('do', 0.008859312484690128)\n",
      "2-gram model: ('doing', 0.0013932196643004046)\n",
      "3-gram model: ('doing', 0.0004067520846044336)\n",
      "4-gram model: ('day', 6.802258349772124e-05)\n"
     ]
    }
   ],
   "source": [
    "my_sentence = \"hey how are you\"\n",
    "my_sentence_tokenized = get_tokenized_data(my_sentence)[0]\n",
    "\n",
    "#the user has entered 'd'\n",
    "user_input = \"d\"\n",
    "suggestions = get_suggestions(my_sentence_tokenized, n_gram_counts_list, vocabulary, k=1.0, start_with=user_input)\n",
    "\n",
    "print(\"The previous words are {}\".format(my_sentence_tokenized))\n",
    "print(\"The suggestions are:\")\n",
    "for i in range(1, n_max + 1, 1):\n",
    "    print(\"{}-gram model: {}\".format(i, suggestions[i - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f436d0",
   "metadata": {},
   "source": [
    "As it is was shown, the heart of n-gram language model is calculating the tables of all encountered tuples of n and n+1. Although this is a simple model and can be implemented and used quickly, the memory consumption can become very large in case of a big training set. \n",
    "\n",
    "Sequence based language models can solve this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f7e00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
