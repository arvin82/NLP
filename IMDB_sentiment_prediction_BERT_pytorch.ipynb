{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a73e72f",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Analysis using BERT transformer in pytorch\n",
    "\n",
    "Loading packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a6c321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel, BertForSequenceClassification\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from ipywidgets import IntProgress\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abbe390",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Data is available from torchdata [Link](https://pytorch.org/data/main/examples.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c4062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = IMDB(split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c44c90",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "\n",
    "Printing a few of the samples from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6941ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment 1: Zentropa has much in common with The Third Man, another noir-like film set among the rubble of postwar Europe. Like TTM, there is much inventive camera work. There is an innocent American who gets emotionally involved with a woman he doesn't really understand, and whose naivety is all the more striking in contrast with the natives.<br /><br />But I'd have to say that The Third Man has a more well-crafted storyline. Zentropa is a bit disjointed in this respect. Perhaps this is intentional: it is presented as a dream/nightmare, and making it too coherent would spoil the effect. <br /><br />This movie is unrelentingly grim--\"noir\" in more than one sense; one never sees the sun shine. Grim, but intriguing, and frightening.\n",
      "Label: pos\n",
      "Comment 2: Zentropa is the most original movie I've seen in years. If you like unique thrillers that are influenced by film noir, then this is just the right cure for all of those Hollywood summer blockbusters clogging the theaters these days. Von Trier's follow-ups like Breaking the Waves have gotten more acclaim, but this is really his best work. It is flashy without being distracting and offers the perfect combination of suspense and dark humor. It's too bad he decided handheld cameras were the wave of the future. It's hard to say who talked him away from the style he exhibits here, but it's everyone's loss that he went into his heavily theoretical dogma direction instead.\n",
      "Label: pos\n",
      "Comment 3: Lars Von Trier is never backward in trying out new techniques. Some of them are very original while others are best forgotten.<br /><br />He depicts postwar Germany as a nightmarish train journey. With so many cities lying in ruins, Leo Kessler a young American of German descent feels obliged to help in their restoration. It is not a simple task as he quickly finds out.<br /><br />His uncle finds him a job as a night conductor on the Zentropa Railway Line. His job is to attend to the needs of the passengers. When the shoes are polished a chalk mark is made on the soles. A terrible argument ensues when a passenger's shoes are not chalked despite the fact they have been polished. There are many allusions to the German fanaticism of adherence to such stupid details.<br /><br />The railway journey is like an allegory representing man's procession through life with all its trials and tribulations. In one sequence Leo dashes through the back carriages to discover them filled with half-starved bodies appearing to have just escaped from Auschwitz . These images, horrible as they are, are fleeting as in a dream, each with its own terrible impact yet unconnected.<br /><br />At a station called Urmitz Leo jumps from the train with a parceled bomb. In view of many by-standers he connects the bomb to the underside of a carriage. He returns to his cabin and makes a connection to a time clock. Later he jumps from the train (at high speed) and lies in the cool grass on a river bank. Looking at the stars above he decides that his job is to build and not destroy. Subsequently as he sees the train approaching a giant bridge he runs at breakneck speed to board the train and stop the clock. If you care to analyse the situation it is a completely impossible task. Quite ridiculous in fact. It could only happen in a dream.<br /><br />It's strange how one remembers little details such as a row of cups hanging on hooks and rattling away with the swaying of the train.<br /><br />Despite the fact that this film is widely acclaimed, I prefer Lars Von Trier's later films (Breaking the Waves and The Idiots). The bomb scene described above really put me off. Perhaps I'm a realist.\n",
      "Label: pos\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "for label, text in data:\n",
    "    print(f\"Comment {count}: \" + text)\n",
    "    print(f\"Label: {label}\")\n",
    "    count += 1\n",
    "    if count == 4: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06179e14",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "First we tokenize all the comments in the dataset and also transform possitive and negative labels to 1 and 0 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50dcdab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "data = IMDB(split='train')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "tokenized_data = tokenizer.batch_encode_plus(\n",
    "    data,\n",
    "    add_special_tokens = True, return_attention_mask = True,\n",
    "    pad_to_max_length = True,\n",
    "    max_length = 256,\n",
    "    return_tensors = 'pt')\n",
    "\n",
    "labels = []\n",
    "for label, _ in data:\n",
    "    if label == 'pos':\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa889db",
   "metadata": {},
   "source": [
    "I had some bugs and conda conflicts trying to use sklearn or torch for splitting. Although it is not probably the most efficient way but I had to split the data to train and test datasets manually. \n",
    "\n",
    "We are using a huggingface transformer based classification model for sequence data [Link](https://huggingface.co/docs/transformers/model_doc/bert). The input format for this data is:\n",
    "- input_ids: which is the tokenized data\n",
    "- attention mask: since the sentences are padded to max_length = 256 the attention mask will be 0 for padded region\n",
    "- labels\n",
    "\n",
    "First we create input_ids, attention_mask, and labels for all the data and then split them into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f2160f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenized_data['input_ids']\n",
    "attention_mask = tokenized_data['attention_mask']\n",
    "labels = labels\n",
    "\n",
    "indices = np.random.permutation(25000)\n",
    "train_indices = indices[:20000]\n",
    "test_indices  = indices[20000:]\n",
    "\n",
    "input_ids_train = torch.stack([input_ids[x] for x in train_indices])\n",
    "attention_mask_train = torch.stack([attention_mask[x] for x in train_indices])\n",
    "labels_train = torch.tensor([labels[x] for x in train_indices])\n",
    "\n",
    "input_ids_test = torch.stack([input_ids[x] for x in test_indices])\n",
    "attention_mask_test = torch.stack([attention_mask[x] for x in test_indices])\n",
    "labels_test = torch.tensor([labels[x] for x in test_indices])\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_mask_train, labels_train)\n",
    "dataset_test  = TensorDataset(input_ids_test , attention_mask_test , labels_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b6c63",
   "metadata": {},
   "source": [
    "Next step is to create batch dataloaders for both both train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abfc917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dataloader = DataLoader(dataset_train, sampler = RandomSampler(dataset_train), batch_size = batch_size)\n",
    "test_dataloader =  DataLoader(dataset_test,  sampler = RandomSampler(dataset_test ), batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3652b61",
   "metadata": {},
   "source": [
    "## Setting up BERT Pretrained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1721daa",
   "metadata": {},
   "source": [
    "Huggingface BERT transformer based model is used for this which may be an overkill!\n",
    "We check if cuda is available and set the model to cuda. Note that if GPU is not available it is unpractical to run this model as it would take >10 hours to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "432017ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2, output_attentions = False,\n",
    "                                      output_hidden_states = False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22ddb7",
   "metadata": {},
   "source": [
    "### Checking the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71847ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (2, 768)\n",
      "classifier.bias                                                 (2,)\n"
     ]
    }
   ],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7aa8704",
   "metadata": {},
   "source": [
    "## Defining Performance Metrics\n",
    "\n",
    "Due to conda conflict with sklearn which could not figure out how to safely manage without installing many packages again, I created F1_score evaluation function below which may not be the most optimum approach.\n",
    "\n",
    "Created one function for F1_score and the other functions returns the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e374216a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "    preds_flat  = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    PC, NC = 0, 0\n",
    "    TP, TN, FP, FN = 0, 0, 0, 0\n",
    "    for pred, label in zip(preds_flat, labels_flat):\n",
    "        if pred == 1 and label == 1:\n",
    "            TP += 1\n",
    "        elif pred == 1 and label == 0:\n",
    "            FP += 1\n",
    "        elif pred == 0 and label == 1:\n",
    "            FN += 1\n",
    "        elif pred == 0 and label == 0:\n",
    "            TN += 1\n",
    "        if label == 1:\n",
    "            PC += 1\n",
    "        else:\n",
    "            NC += 1\n",
    "    \n",
    "    precision = TP / (TP + FP)\n",
    "    recall    = TP / (TP + FN)\n",
    "    print(\"TP:{}, TN:{}, FP:{}, FN:{}\".format(TP, TN, FP, FN))\n",
    "    print(\"Precision:{}, Recall:{}\".format(precision, recall))\n",
    "    print(\"Positive cases: {}, Negative cases: {}\".format(PC, NC))\n",
    "    return 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b7ccfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_per_class(preds, labels):\n",
    "    preds_flat  = np.argmax (preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    inverse_dict = {1:'pos', 0:'neg'}\n",
    "    \n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true  = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {inverse_dict[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d6f6de",
   "metadata": {},
   "source": [
    "The 'evaluate(...)' functions receives the dataloader of a dataset and returns the average loss together with predicted labels from the model. The gradients are not calculated when we just want to forward the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c0fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in tqdm(dataloader_val):\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_total, predictions, true_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4979932",
   "metadata": {},
   "source": [
    "### Checking the performance of untrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d85802",
   "metadata": {},
   "source": [
    "Since the model is very strong for this application and we are using pre-trained BERT with word embeddings, we want to first see how the model output performace is even without any prior training on our dataset. \n",
    "\n",
    "For this we send test dataset to the 'evaluate' function and output the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259dca2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be55e3e1ce1f45c5b679a3c42f4ffda4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:1214, TN:1272, FP:1219, FN:1295\n",
      "Precision:0.4989724619810933, Recall:0.48385811080111596\n",
      "Positive cases: 2509, Negative cases: 2491\n",
      "Validation loss: 109.1338484287262\n",
      "F1 score: 0.4912990692027519\n",
      "Evaluation took 20.51658296585083 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "val_loss, predictions, true_vals = evaluate(test_dataloader)\n",
    "T = time.time() - start\n",
    "val_f1 = f1_score_func(predictions, true_vals)\n",
    "tqdm.write(f'Validation loss: {val_loss}')\n",
    "tqdm.write(f'F1 score: {val_f1}')\n",
    "tqdm.write(f'Evaluation took {T} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ffc23",
   "metadata": {},
   "source": [
    "It seems that the data is balanced and there is relatively the same number of possible and negative labels. However, it appears that the untrained models predicts all samples as possitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76657c3",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "First we setup the optimizer and the learning rate schedulers. And we set the model for only 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f451f1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5, eps = 1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = 25000 * epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aad11a6",
   "metadata": {},
   "source": [
    "Training loop is simple. For each batch, we get the input_ids, attention_mask, and labels as defined earlier for the train dataloader. We send that to the model and accumulate the returned loss.\n",
    "\n",
    "After the training loop is finished going over all batches and epochs, we call the evaluate on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2a88c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Training Loss: 26.96146459034935\n",
      "Now evaluating the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e49e967c854d56a0d62161c9917ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:2509, TN:2491, FP:0, FN:0\n",
      "Precision:1.0, Recall:1.0\n",
      "Positive cases: 2509, Negative cases: 2491\n",
      "Validation loss: 0.02230093772232067\n",
      "F1 score: 1.0\n",
      "Training took 5.007192858060201 minutes.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(1, epochs+1):\n",
    "    \n",
    "    model.train()\n",
    "    loss_train_total = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc='Epoch {:1d}'.format(epoch), leave = False, disable = False)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask':batch[1], 'labels':batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({f'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "    \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    tqdm.write(f'Training Loss: {loss_train_total}')\n",
    "    \n",
    "    print(\"Now evaluating the test set...\")\n",
    "    val_loss, predictions, true_vals = evaluate(test_dataloader)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 score: {val_f1}')\n",
    "\n",
    "tqdm.write(f'Training took {(time.time() - start)/60} minutes.')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce1052",
   "metadata": {},
   "source": [
    "It seems the model does not make any mistakes! That is probably because the train and test datasets are very similar to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa342bc",
   "metadata": {},
   "source": [
    "## Evaluating on custom input\n",
    "\n",
    "Now we can use the trained model and input any custom sentence to it and evaluate its out put. We can define a function to evaluate single comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e8755a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_dict = {1:'pos', 0:'neg'}\n",
    "def evaulate_comment(comment):\n",
    "    inputs = tokenizer(comment, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "    labels.to(device)\n",
    "    with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "    logits = outputs[\"logits\"]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    category  = np.argmax(logits, axis = 1).flatten()\n",
    "    return inverse_dict[category[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853af3e9",
   "metadata": {},
   "source": [
    "We call this function for a list of comments below that can be modified. The model performs really good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cb8f343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the comment:Too many movies like this\t Prediction is: neg\n",
      "For the comment:Too long for such a move!\t Prediction is: neg\n",
      "For the comment:Lucky that he is still making movies.\t Prediction is: pos\n",
      "For the comment:A movie for everyone, no matter what age.\t Prediction is: pos\n"
     ]
    }
   ],
   "source": [
    "comments = [\"Too many movies like this\",\n",
    "            \"Too long for such a move!\",\n",
    "            \"Lucky that he is still making movies.\",\n",
    "            \"A movie for everyone, no matter what age.\",\n",
    "            \n",
    "           ]\n",
    "\n",
    "for comment in comments:\n",
    "    print(\"For the comment:\" + comment + \"\\t Prediction is: \" + evaulate_comment(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358cbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
